{"config": {"view": {"continuousWidth": 300, "continuousHeight": 300}}, "data": {"name": "data-5b3a80f8e50b5f166bfb765de8d6e42e"}, "mark": {"type": "circle", "size": 60}, "encoding": {"tooltip": [{"field": "text", "type": "nominal"}], "x": {"field": "x", "type": "quantitative"}, "y": {"field": "y", "type": "quantitative"}}, "title": "Text Embeddings Visualization", "$schema": "https://vega.github.io/schema/vega-lite/v5.17.0.json", "datasets": {"data-5b3a80f8e50b5f166bfb765de8d6e42e": [{"x": 2.464536190032959, "y": -9.335540771484375, "text": "[Page no. 1] \"PyTorch: An Imperative Style, High-Performance Deep Learning Library Adam Paszke University of Warsaw adam.paszke@gmail.com Sam Gross Facebook AI Research sgross@fb.com Francisco Massa Facebook AI Research fmassa@fb.com Adam Lerer Facebook AI Research alerer@fb.com James Bradbury Google jekbradbury@gmail.com Gregory Chanan Facebook AI Research gchanan@fb.com Trevor Killeen Self Employed killeent@cs.washington.edu Zeming Lin Facebook AI Research zlin@fb.com Natalia Gimelshein NVIDIA ngimelshein@nvidia.com Luca Antiga Orobix luca.antiga@orobix.com Alban Desmaison Oxford University alban@robots.ox.ac.uk Andreas K\u00f6pf Xamla andreas.koepf@xamla.com Edward Yang Facebook AI Research ezyang@fb.com Zach DeVito Facebook AI Research zdevito@cs.stanford.edu Martin Raison Nabla martinraison@gmail.com Alykhan Tejani Twitter atejani@twitter.com Sasank Chilamkurthy Qure.ai sasankchilamkurthy@gmail.com Benoit Steiner Facebook AI Research benoitsteiner@fb.com Lu Fang Facebook lufang@fb.com Junjie Bai Facebook jbai@fb.com Soumith Chintala Facebook AI Research soumith@gmail.com Abstract Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and\""}, {"x": -0.610438883304596, "y": -8.00868034362793, "text": "[Page no. 2] \"Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scienti\ufb01c computing libraries, while remaining ef\ufb01cient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are re\ufb02ected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the ef\ufb01ciency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks. 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada. arXiv:1912.01703v1 [cs.LG] 3 Dec 2019  1 Introduction With the increased interest in deep learning in recent years, there has been an explosion of\""}, {"x": 0.06533392518758774, "y": -7.210832595825195, "text": "[Page no. 2] \"machine learning tools. Many popular frameworks such as Caffe [1], CNTK [2], TensorFlow [3], and Theano [4], construct a static data\ufb02ow graph that represents the computation and which can then be applied repeatedly to batches of data. This approach provides visibility into the whole computation ahead of time, and can theoretically be leveraged to improve performance and scalability. However, it comes at the cost of ease of use, ease of debugging, and \ufb02exibility of the types of computation that can be represented. Prior work has recognized the value of dynamic eager execution for deep learning, and some recent frameworks implement this de\ufb01ne-by-run approach, but do so either at the cost of performance (Chainer [5]) or using a less expressive, faster language (Torch [6], DyNet [7]), which limits their applicability. However, with careful implementation and design choices, dynamic eager execution can be achieved largely without sacri\ufb01cing performance. This paper introduces PyTorch,\""}, {"x": -0.9920269250869751, "y": -9.665054321289062, "text": "[Page no. 2] \"a Python library that performs immediate execution of dynamic tensor computations with automatic differentiation and GPU acceleration, and does so while maintaining performance comparable to the fastest current libraries for deep learning. This combination has turned out to be very popular in the research community with, for instance, 296 ICLR 2019 submissions mentioning PyTorch. 2 Background Four major trends in scienti\ufb01c computing have become increasingly important for deep learning. First, starting in the 1960s, the development of domain speci\ufb01c languages such as APL [8], MATLAB [9], R [10] and Julia [11], turned multidimensional arrays (often referred to as tensors) into \ufb01rst-class objects supported by a comprehensive set of mathematical primitives (or operators) to manipulate them. Separately, libraries such as NumPy[12], Torch[6], Eigen[13] and Lush[14] made array-based programming productive in general purpose languages such as Python, Lisp, C++ and Lua. Second, the development of automatic differentiation [15] made it possible to\""}, {"x": -0.8499342799186707, "y": -11.25644302368164, "text": "[Page no. 2] \"fully automate the daunting labor of computing derivatives. This made it signi\ufb01cantly easier to experiment with different machine learning approaches while still allowing for ef\ufb01cient gradient based optimization. The autograd [16] package popularized the use of this technique for NumPy arrays, and similar approaches are used in frameworks such as Chainer [5], DyNet [7], Lush [14], Torch [6], Jax [17] and Flux.jl [18]. Third, with the advent of the free software movement, the scienti\ufb01c community moved away from closed proprietary software such as Matlab[9], and towards the open-source Python ecosystem with packages like NumPy [12], SciPy [19], and Pandas [20]. This ful\ufb01lled most of the numerical analysis needs of researchers while allowing them to take advantage of a vast repository of libraries to handle dataset preprocessing, statistical analysis, plotting, and more. Moreover, the openness, interoperability, and \ufb02exibility of free software fostered the development of vibrant communities that could quickly address\""}, {"x": 0.8643813729286194, "y": -7.776119232177734, "text": "[Page no. 2] \"new or changing needs by extending the existing functionality of a library or if needed by developing and releasing brand new ones. While there is a rich offering of open-source software for neural networks in languages other than Python, starting with Lush [14] in Lisp, Torch [6] in C++, Objective-C and Lua, EBLearn [21] in C++, Caffe [1] in C++, the network effects of a large ecosystem such as Python made it an essential skill to jumpstart one\u2019s research. Hence, since 2014, most deep learning frameworks converged on a Python interface as an essential feature. Finally, the availability and commoditization of general-purpose massively parallel hardware such as GPUs provided the computing power required by deep learning methods. Specialized libraries such as cuDNN [22], along with a body of academic work (such as [23] and [24]), produced a set of high-performance reusable deep learning kernels that enabled frameworks such as Caffe\""}, {"x": -1.2263085842132568, "y": -8.855374336242676, "text": "[Page no. 3] \"[1], Torch7 [25], or TensorFlow [3] to take advantage of these hardware accelerators. PyTorch builds on these trends by providing an array-based programming model accelerated by GPUs and differentiable via automatic differentiation integrated in the Python ecosystem. 2  3 Design principles PyTorch\u2019s success stems from weaving previous ideas into a design that balances speed and ease of use. There are four main principles behind our choices: Be Pythonic Data scientists are familiar with the Python language, its programming model, and its tools. PyTorch should be a \ufb01rst-class member of that ecosystem. It follows the commonly established design goals of keeping interfaces simple and consistent, ideally with one idiomatic way of doing things. It also integrates naturally with standard plotting, debugging, and data processing tools. Put researchers \ufb01rst PyTorch strives to make writing models, data loaders, and optimizers as easy and productive as possible. The complexity inherent to machine learning\""}, {"x": -2.1648526191711426, "y": -8.68290901184082, "text": "[Page no. 3] \"should be handled internally by the PyTorch library and hidden behind intuitive APIs free of side-effects and unexpected performance cliffs. Provide pragmatic performance To be useful, PyTorch needs to deliver compelling performance, although not at the expense of simplicity and ease of use. Trading 10% of speed for a signi\ufb01cantly simpler to use model is acceptable; 100% is not. Therefore, its implementation accepts added complexity in order to deliver that performance. Additionally, providing tools that allow researchers to manually control the execution of their code will empower them to \ufb01nd their own performance improvements independent of those that the library provides automatically. Worse is better [26] Given a \ufb01xed amount of engineering resources, and all else being equal, the time saved by keeping the internal implementation of PyTorch simple can be used to implement additional features, adapt to new situations, and keep up with the fast pace of progress in\""}, {"x": 0.05362905561923981, "y": -9.020552635192871, "text": "[Page no. 3] \"the \ufb01eld of AI. Therefore it is better to have a simple but slightly incomplete solution than a comprehensive but complex and hard to maintain design. 4 Usability centric design 4.1 Deep learning models are just Python programs In a surprisingly short amount of time, machine learning grew from recognizing individual digits [27] into autonomously playing StarCraft [28]. Consequently, the neural networks themselves evolved rapidly from simple sequences of feed forward layers into incredibly varied numerical programs often composed of many loops and recursive functions. To support this growing complexity, PyTorch foregoes the potential bene\ufb01ts of a graph-metaprogramming based approach to preserve the imperative programming model of Python. This design was pioneered for model authoring by Chainer[5] and Dynet[7]. PyTorch extends this to all aspects of deep learning work\ufb02ows. De\ufb01ning layers, composing models, loading data, running optimizers, and parallelizing the training process are all expressed using the familiar concepts developed\""}, {"x": 0.43032801151275635, "y": -9.261970520019531, "text": "[Page no. 4] \"for general purpose programming. This solution ensures that any new potential neural network architecture can be easily implemented with PyTorch. For instance, layers (which in modern machine learning should really be understood as stateful functions with implicit parameters) are typically expressed as Python classes whose constructors create and initialize their parameters, and whose forward methods process an input activation. Similarly, models are usually represented as classes that compose individual layers, but let us state again that nothing forces the user to structure their code in that way. Listing 1 demonstrates how an entire model can be created by composing functionality provided by PyTorch such as 2d convolution, matrix multiplication, dropout, and softmax to classify gray-scale images. Note that linear layers are of course part of the library, but we show an example implementation to highlight how simple it is. 3  class LinearLayer(Module): class FullBasicModel(nn.Module): def __init__(self, in_sz, out_sz): def\""}, {"x": 0.7877566814422607, "y": -10.1734619140625, "text": "[Page no. 4] \"__init__(self): super().__init__() super().__init__() t1 = torch.randn(in_sz, out_sz) self.conv = nn.Conv2d(1, 128, 3) self.w = nn.Parameter(t1) self.fc = LinearLayer(128, 10) t2 = torch.randn(out_sz) self.b = nn.Parameter(t2) def forward(self, x): t1 = self.conv(x) def forward(self, activations): t2 = nn.functional.relu(t1) t = torch.mm(activations, self.w) t3 = self.fc(t1) return t + self.b return nn.functional.softmax(t3) Listing 1: A custom layer used as a building block for a simple but complete neural network. This \u201ceverything is a just a program\u201d philosophy is not limited to just the models, and applies to optimizers and data loaders as well. This facilitates the experimentation of new training techniques. For example, to implement the very popular generative adversarial networks, one needs to specify two separate models (the generator and the discriminator), and two loss functions that depend on both models at the same time. Rigid APIs would struggle with this setup, but the simple design employed in PyTorch easily adapts\""}, {"x": 0.2145668864250183, "y": -10.260141372680664, "text": "[Page no. 4] \"to this setting as shown in Listing 2. discriminator = create_discriminator() generator = create_generator() optimD = optim.Adam(discriminator.parameters()) optimG = optim.Adam(generator.parameters()) def step(real_sample): # (1) Update Discriminator errD_real = loss(discriminator(real_sample), real_label) errD_real.backward() fake = generator(get_noise()) errD_fake = loss(discriminator(fake.detach(), fake_label) errD_fake.backward() optimD.step() # (2) Update Generator errG = loss(discriminator(fake), real_label) errG.backward() optimG.step() Listing 2: Simpli\ufb01ed training of a generative adversarial networks. Since PyTorch programs execute eagerly, all the features of Python are available throughout the whole design process. Print statements, standard debuggers, and common visualization tools like matplotlib all work as expected. Users do not have to wait for lengthy compilation before they can start running their programs, and more importantly intermediate computations can be observed to understand how a model works and whether its results are correct. 4.2 Interoperability and extensibility Easy and ef\ufb01cient interoperability is one of the top priorities for PyTorch because it opens the possibility to leverage the\""}, {"x": -2.6864895820617676, "y": -7.447791576385498, "text": "[Page no. 5] \"rich ecosystem of Python libraries as part of user programs. Hence, PyTorch allows for bidirectional exchange of data with external libraries. For example, it provides a mechanism to convert between NumPy arrays and PyTorch tensors using the torch.from_numpy() function and .numpy() tensor method. Similar functionality is also available to exchange data stored using the DLPack [29] format. Note that this exchange happens in both cases without any data copying \u2013 objects on both sides only describe how to interpret a memory region which is shared among them. Hence, those operations are actually extremely cheap, and take constant time no matter how large the converted arrays are. 4  Moreover, many of the critical systems are designed speci\ufb01cally to be extensible. For instance, the automatic differentiation system allows users to add support for custom differentiable functions. To do that users can de\ufb01ne a new subclass of torch.autograd.Function that implements forward() and\""}, {"x": -2.0798234939575195, "y": -7.100935935974121, "text": "[Page no. 5] \"backward() methods, which specify the function and its derivative (or more formally the vector- Jacobian product). Similarly new datasets can be added by subclassing torch.utils.data.Dataset and implementing two methods: __getitem__ (the indexing operator) and __len__ (the length op- erator), making datasets behave like (possibly lazy) lists. How these work is completely up to the implementer, and many users leverage other Python packages for data loading. The DataLoader class consumes objects conforming to this interface and provides an iterator over the data which takes care of shuf\ufb02ing, batching, parallelization, and management of pinned CUDA memory to improve throughput. Most importantly, users are free to replace any component of PyTorch that does not meet the needs or performance requirements of their project. They are all designed to be completely interchangeable, and PyTorch takes great care not to impose any particular solution. 4.3 Automatic differentiation Since gradient based optimization is vital to deep\""}, {"x": -1.8308465480804443, "y": -9.684185981750488, "text": "[Page no. 5] \"learning, PyTorch must be able to automatically compute gradients of models speci\ufb01ed by our users, and those can be arbitrary Python programs. However, Python is a dynamic programming language that allows changing most behaviors at runtime, making ahead of time source-to-source differentiation cumbersome. Instead, PyTorch uses the operator overloading approach, which builds up a representation of the computed function every time it is executed. In its current implementation [30], PyTorch performs reverse-mode automatic differentiation, which computes the gradient of a scalar output with respect to a multivariate input. Differentiating functions with more outputs than inputs is more ef\ufb01ciently executed using forward- mode automatic differentiation, but this use case is less common for machine learning applications. PyTorch can be easily extended to perform forward-mode differentiation using array-level dual numbers [31, 32]. Another interesting and uncommon feature of our system is that it can differentiate through code employing mutation on tensors, which\""}, {"x": -4.047962188720703, "y": -7.848894119262695, "text": "[Page no. 5] \"is one of the basic building blocks of imperative programs. To ensure safety, we have implemented a versioning system for tensors, which lets us track their modi\ufb01cations and ensure that we always use the data we expect. One interesting tradeoff is that while we could utilize techniques like copy-on-write to support arbitrary programs, we chose to not go down this path, as performance-wise it is usually bene\ufb01cial for the users to rewrite their code to ensure that no copies have to be performed. Hence, while most mutations are benign and can be handled automatically, the really complicated cases result in a user error, which lets them know that they likely want to restructure the program. This allows us to avoid introducing subtle and hard-to-\ufb01nd performance cliffs. 5 Performance focused implementation Running deep learning algorithms ef\ufb01ciently from a Python interpreter is notoriously challenging: for instance, the global interpreter lock [33] effectively\""}, {"x": -1.2720186710357666, "y": -7.999608039855957, "text": "[Page no. 5] \"ensures that only one of any number of concurrent threads is running at any given time. Deep learning frameworks based on the construction of a static data-\ufb02ow graph sidestep this problem by deferring the evaluation of the computation to a custom interpreter. PyTorch solved the problem differently, by carefully optimizing every aspect of its execution while simultaneously empowering its users to easily leverage additional optimization strategies. 5.1 An ef\ufb01cient C++ core Despite being closely integrated in the Python ecosystem, most of PyTorch is written in C++ to achieve high performance. This core libtorch library implements the tensor data structure, the GPU and CPU operators, and basic parallel primitives. It also provides the automatic differentiation system, including the gradient formulas for most built-in functions. This ensures that the computation of the derivatives of functions composed of core PyTorch operators is executed entirely in a multithreaded evaluator which does not require holding\""}, {"x": -3.169184684753418, "y": -8.308445930480957, "text": "[Page no. 6] \"the Python global interpreter lock [33]. Python bindings 5  are generated using YAML meta-data \ufb01les. An interesting side-effect of this approach is that it allowed our community to quickly create bindings to multiple other languages resulting in projects like NimTorch [34], hasktorch [35] and others. This design also allowed us to create \ufb01rst-class C++ bindings and modeling libraries that can be used in places where Python is inconvenient, such as the game engine for Starcraft [36] or on mobile platforms. It is even possible to take the Python code describing a PyTorch model and run it without Python using the TorchScript engine [37]. 5.2 Separate control and data \ufb02ow PyTorch maintains a strict separation between its control (i.e. program branches, loops) and data \ufb02ow (i.e. tensors and the operations performed on them). The resolution of the control \ufb02ow is handled by Python and optimized C++ code executed on the\""}, {"x": -1.8213422298431396, "y": -5.341081619262695, "text": "[Page no. 6] \"host CPU, and result in a linear sequence of operator invocations on the device. Operators can be run either on CPU or on GPU. PyTorch is designed to execute operators asynchronously on GPU by leveraging the CUDA stream mechanism [38] to queue CUDA kernel invocations to the GPUs hardware FIFO. This allows the system to overlap the execution of Python code on CPU with tensor operators on GPU. Because the tensor operations usually take a signi\ufb01cant amount of time, this lets us saturate the GPU and reach peak performance even in an interpreted language with fairly high overhead like Python. Note that this mechanism is nearly invisible to the user. Unless they implement their own multi-stream primitives all of the CPU-GPU synchronization is handled by the library. PyTorch could leverage a similar mechanism to also execute operators asynchronously on the CPU. However the costs of cross-thread communication and synchronization would\""}, {"x": -2.0704774856567383, "y": -4.682408809661865, "text": "[Page no. 6] \"negate the performance bene\ufb01t of such an optimization. 5.3 Custom caching tensor allocator Almost every operator must dynamically allocate an output tensor to hold the result of its execution. It is therefore critical to optimize the speed of the dynamic memory allocators. PyTorch can rely on optimized libraries [39\u201341] to handle this task on CPU. However, on GPU the cudaFree routine may block its caller until all previously queued work on all GPUs completes. To avoid this bottleneck, PyTorch implements a custom allocator which incrementally builds up a cache of CUDA memory and reassigns it to later allocations without further use of CUDA APIs. The incremental allocation is also crucial for better interoperability, because taking up all GPU memory ahead of time would prevent the user from utilizing other GPU-enabled Python packages. To further improve its effectiveness, this allocator was tuned for the speci\ufb01c memory usage patterns of deep learning.\""}, {"x": -2.7891902923583984, "y": -3.3866963386535645, "text": "[Page no. 6] \"For example, it rounds up allocations to multiples of 512 bytes to avoid fragmentation issues. Moreover, it maintains a distinct pool of memory for every CUDA stream (work queue). The one-pool-per-stream design assumption simpli\ufb01es the implementation and improves the perfor- mance of the allocator: because the CPU runs ahead of the GPU, memory is freed on the CPU before its last use on the GPU \ufb01nishes. Since streams serialize execution, if the free precedes the reallocation on the CPU, the same order will occur on the GPU. So the allocator can reallocate memory freed on the CPU immediately as long as the new allocation is used on the same stream as the freed region. However, if an allocation was last used on one stream and then allocated on another, additional synchronization is needed. The one-pool-per-stream design seems limiting since the allocations end up fragmented per stream, but in practice PyTorch\""}, {"x": -3.0171709060668945, "y": -4.53645133972168, "text": "[Page no. 7] \"almost never uses multiple streams. It is notoriously hard to write CUDA kernels in a way that would let them cooperatively share the GPU because exact scheduling is hardware controlled. In practice, kernel writers usually resort to monolithic kernels that combine multiple tasks. Data loading and distributed computing utilities are exceptions to the one stream design, and they carefully insert additional synchronization to avoid bad interactions with the allocator. While this design is susceptible to certain corner cases, it almost never exhibits unwanted behaviors in practical code. Most of our users are not aware of its existence. 6  5.4 Multiprocessing Due to the global interpreter lock (GIL) Python\u2019s default implementation does not allow concurrent threads to execute in parallel. To alleviate this problem, the Python community has established a standard multiprocessing module, containing a number of utilities that allow users to easily spawn child processes and implement basic inter-process\""}, {"x": -2.268639087677002, "y": -5.850104331970215, "text": "[Page no. 7] \"communication primitives. However, the implementation of the primitives uses the same form of serialization used for on-disk persistence, which is inef\ufb01cient when dealing with large arrays. Hence, PyTorch extends the Python multiprocessing module into torch.multiprocessing, which is a drop-in replacement for the built in package and automatically moves the data of tensors sent to other processes to shared memory instead of sending it over the communication channel. This design greatly improves performance and makes the process isolation weaker, resulting in a programming model which more closely resembles regular threaded programs. Users can easily implement heavily parallel programs that operate on independent GPUs but later synchronize gradients using all-reduce style primitives. Another unique feature of this system is that it transparently handles sharing of CUDA tensors, making it easy to implement techniques like Hogwild [42]. 5.5 Reference counting Users often design their models to utilize all memory available during training, and\""}, {"x": -3.102292537689209, "y": -5.785648822784424, "text": "[Page no. 7] \"increasing batch sizes is a common technique of speeding up the process. Therefore, to deliver great performance, PyTorch has to treat memory as a scarce resource that it needs to manage carefully. Libraries with eager semantics have to manage tensor memory without knowing how it will be used in the future. Garbage collection is the typical way to handle this automatically because it has good amortized performance. In this approach, the runtime periodically investigates the state of the system, enumerates used objects and frees everything else. However, by deferring the deallocation, it causes the program to use more memory overall [43]. Given the scarcity of GPU memory, these overheads are unacceptable. In fact, Torch7 utilized the garbage collector built into Lua, and a common anti- pattern among the users was to sprinkle the program with explicit triggers to the garbage collector, hoping that the memory errors go away. PyTorch takes\""}, {"x": -3.612048864364624, "y": -6.454200744628906, "text": "[Page no. 7] \"a different approach: it relies on a reference counting scheme to track the number of uses of each tensor, and frees the underlying memory immediately once this count reaches zero. Note that PyTorch tracks both references internal to the libtorch library and external references made by users in their Python code by integrating with Python\u2019s own reference counting mechanism. This ensures that memory is released exactly when tensors become unneeded. One notable caveat is that we can only guarantee the desired performance characteristics in implemen- tations of languages that either already utilize reference counting (CPython, Swift, but not PyPy or many scripting languages such as Lua), and those that allow for user-de\ufb01ned behavior for assignment, copies, and moves (e.g. C++, Rust). Bindings to implementations that do not satisfy those criteria will have to implement their own specialized memory management on top of PyTorch. 6 Evaluation In this section we compare\""}, {"x": -0.3724002242088318, "y": -5.449999809265137, "text": "[Page no. 8] \"the performance of PyTorch with several other commonly-used deep learning libraries, and \ufb01nd that it achieves competitive performance across a range of tasks. All experiments were performed on a workstation with two Intel Xeon E5-2698 v4 CPUs and one NVIDIA Quadro GP100 GPU. 6.1 Asynchronous data\ufb02ow We start by quantifying the ability of PyTorch to asynchronously execute data\ufb02ow on GPU. We use the built-in pro\ufb01ler [44] to instrument various benchmarks and record a timeline of the execution of a single training step. 7  Figure 1 shows a representative timeline of execution for the \ufb01rst few operations of a ResNet-50 model. The host CPU which queues the work quickly outpaces the execution of the operators on the GPU. This allows PyTorch to achieve almost perfect device utilization. In this example, GPU execution takes around three times longer than CPU scheduling. The exact ratio depends on the relative performance of the\""}, {"x": -1.0264674425125122, "y": -4.2204270362854, "text": "[Page no. 8] \"host CPU and the GPU, as well as the number of elements in each tensor and the average arithmetic complexity of the \ufb02oating point computations to be performed on the GPU. Figure 1: A trace of the \ufb01rst few operators of Resnet-50. The top row depicts the execution of the control \ufb02ow running on the host CPU. The gray areas are Python code executed by its interpreter. The colored areas correspond to the work done on the host CPU to queue various operators (convolution, batch normalization, and so on). The bottom row shows the corresponding execution of those operators on the GPU. The arrows pair the two events in time. 6.2 Memory management We used the NVIDIA pro\ufb01ler to trace the execution of the CUDA runtime as well as the execution of the CUDA kernels launched during one training iteration of the ResNet-50 model. As shown in Figure 2, the\""}, {"x": -0.3744698464870453, "y": -5.620034694671631, "text": "[Page no. 8] \"behavior of the \ufb01rst iteration differs signi\ufb01cantly from that of subsequent ones. At \ufb01rst, calls to the CUDA memory management functions (cudaMalloc and cudaFree) slow down the execution quite dramatically by blocking the CPU thread for long periods of time, hence lowering the utilization of the GPU. This effect disappears in subsequent iterations as the PyTorch caching memory allocator starts reusing previously allocated regions. Figure 2: Annotated traces of the execution of ResNet-50 on GPU. 6.3 Benchmarks Finally, we can get an overall sense of single-machine eager mode performance of PyTorch by com- paring it to three popular graph-based deep learning frameworks (CNTK, MXNet and TensorFlow), a de\ufb01ne-by-run framework (Chainer), and production oriented platform (PaddlePaddle). The Appendix details all the steps needed to reproduce our setup. Our results are summarized in Table 1. On all the benchmarks, the performance of PyTorch is within 17% of that of of the fastest\""}, {"x": 1.1037875413894653, "y": -5.961210250854492, "text": "[Page no. 8] \"framework. We attribute this result to the fact that these tools of\ufb02oad most of the computation to the same version of the cuDNN and cuBLAS libraries. Framework Throughput (higher is better) AlexNet VGG-19 ResNet-50 MobileNet GNMTv2 NCF Chainer 778 \u00b1 15 N/A 219 \u00b1 1 N/A N/A N/A CNTK 845 \u00b1 8 84 \u00b1 3 210 \u00b1 1 N/A N/A N/A MXNet 1554 \u00b1 22 113 \u00b1 1 218 \u00b1 2 444 \u00b1 2 N/A N/A PaddlePaddle 933 \u00b1 123 112 \u00b1 2 192 \u00b1 4 557 \u00b1 24 N/A N/A TensorFlow 1422 \u00b1 27 66 \u00b1 2 200 \u00b1 1 216 \u00b1 15 9631 \u00b1 1.3% 4.8e6 \u00b1 2.9% PyTorch 1547 \u00b1 316 119 \u00b1 1 212 \u00b1 2 463 \u00b1 17 15512 \u00b1 4.8% 5.4e6 \u00b1 3.4% Table 1: Training speed for 6 models using 32bit \ufb02oats. Throughput is measured in images per second for the AlexNet, VGG-19,\""}, {"x": 1.4789232015609741, "y": -6.484119415283203, "text": "[Page no. 9] \"ResNet-50, and MobileNet models, in tokens per second for the GNMTv2 model, and in samples per second for the NCF model. The fastest speed for each model is shown in bold. 8  6.4 Adoption The validity of design decisions and their impact on ease-of-use is hard to measure. As a proxy, we tried to quantify how well the machine learning community received PyTorch by counting how often various machine learning tools (including Caffe, Chainer, CNTK, Keras, MXNet, PyTorch, TensorFlow, and Theano) are mentioned on arXiv e-Prints since the initial release of PyTorch in January 2017. In Figure 3 we report the monthly number of mentions of the word \"PyTorch\" as a percentage of all mentions among these deep learning frameworks. We counted tools mentioned multiple times in a given paper only once, and made the search case insensitive to account for various spellings. Figure 3: Among arXiv papers each\""}, {"x": 0.05795304477214813, "y": -7.861016750335693, "text": "[Page no. 9] \"month that mention common deep learning frameworks, percentage of them that mention PyTorch. 7 Conclusion and future work PyTorch has become a popular tool in the deep learning research community by combining a focus on usability with careful performance considerations. In addition to continuing to support the latest trends and advances in deep learning, in the future we plan to continue to improve the speed and scalability of PyTorch. Most notably, we are working on the PyTorch JIT: a suite of tools that allow PyTorch programs to be executed outside of the Python interpreter where they can be further optimized. We also intend to improve support for distributed computation by providing ef\ufb01cient primitives for data parallelism as well as a Pythonic library for model parallelism based around remote procedure calls. 8 Acknowledgements We are grateful to the PyTorch community for their feedback and contributions that greatly in\ufb02uenced the design and\""}, {"x": -3.5985376834869385, "y": -9.541780471801758, "text": "[Page no. 9] \"implementation of PyTorch. We thank all the PyTorch core team members, contributors and package maintainers including Ailing Zhang, Alex Suhan, Alfredo Mendoza, Alican Bozkurt, Andrew Tulloch, Ansha Yu, Anthony Shoumikhin, Bram Wasti, Brian Vaughan, Christian Puhrsch, David Reiss, David Riazati, Davide Libenzi, Dmytro Dzhulgakov, Dwaraj Rajagopal, Edward Yang, Elias Ellison, Fritz Obermeyer, George Zhang, Hao Lu, Hong Xu, Hung Duong, Igor Fedan, Ilia Cherniavskii, Iurii Zdebskyi, Ivan Kobzarev, James Reed, Jeff Smith, Jerry Chen, Jerry Zhang, Jiakai Liu, Johannes M. Dieterich, Karl Ostmo, Lin Qiao, Martin Yuan, Michael Suo, Mike Ruberry, Mikhail Zolothukhin, Mingzhe Li, Neeraj Pradhan, Nick Korovaiko, Owen Anderson, Pavel Belevich, Peter Johnson, Pritam Damania, Raghuraman Krishnamoorthi, Richard Zou, Roy Li, Rui Zhu, Sebastian Messmer, Shen Li, Simon Wang, Supriya Rao, Tao Xu, Thomas Viehmann, Vincent Quenneville- Belair, Vishwak Srinivasan, Vitaly Fedyunin, Wanchao Liang, Wei Yang, Will Feng, Xiaomeng Yang, Xiaoqiang Zheng, Xintao Chen, Yangqing Jia, Yanli Zhao,\""}, {"x": 2.7070136070251465, "y": -7.372741222381592, "text": "[Page no. 10] \"Yinghai Lu and Zafar Takhirov. References [1] Yangqing \"Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor\" Darrell. \"caffe: Convolutional architecture for fast feature embedding\". \"arXiv preprint arXiv:1408.5093\", \"2014\". [2] Frank Seide and Amit Agarwal. Cntk: Microsoft\u2019s open-source deep-learning toolkit. In Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201916, pages 2135\u20132135, New York, NY, USA, 2016. ACM. 9  [3] Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Man\u00e9, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi\u00e9gas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin\""}, {"x": 1.7223002910614014, "y": -8.181581497192383, "text": "[Page no. 10] \"Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large- scale machine learning on heterogeneous systems, 2015. Software available from tensor\ufb02ow.org. [4] Theano Development Team. Theano: A Python framework for fast computation of mathematical expressions. arXiv e-prints, abs/1605.02688, May 2016. [5] Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton. Chainer: a next-generation open source framework for deep learning. In Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS), 2015. [6] Ronan Collobert, Samy Bengio, and Johnny Mari\u00e9thoz. Torch: a modular machine learning software library. Technical report, Idiap, 2002. [7] G. Neubig, C. Dyer, Y. Goldberg, A. Matthews, W. Ammar, A. Anastasopoulos, M. Balles- teros, D. Chiang, D. Clothiaux, T. Cohn, K. Duh, M. Faruqui, C. Gan, D. Garrette, Y. Ji, L. Kong, A. Kuncoro, G. Kumar, C. Malaviya, P. Michel, Y. Oda, M. Richardson, N. Saphra, S. Swayamdipta, and P. Yin.\""}, {"x": 0.837309718132019, "y": -11.734295845031738, "text": "[Page no. 10] \"DyNet: The Dynamic Neural Network Toolkit. ArXiv e-prints, January 2017. [8] Philip S. Abrams. An APL Machine. PhD thesis, Stanford University, 1970. [9] The MathWorks, Inc., Natick, Massachusetts, United States. MATLAB and Statistics Toolbox. [10] R Core Team. R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria. [11] Jeff Bezanson, Alan Edelman, Stefan Karpinski, and Viral B Shah. Julia: A fresh approach to numerical computing. SIAM review, 59(1):65\u201398, 2017. [12] Travis Oliphant. NumPy: A guide to NumPy. USA: Trelgol Publishing, 2006. http://www.numpy.org/. [13] Ga\u00ebl Guennebaud, Beno\u00eet Jacob, et al. Eigen v3. http://eigen.tuxfamily.org, 2010. [14] Y LeCun and L Bottou. Lush reference manual. Technical report, code available at http://lush.sourceforge.net, 2002. [15] Atilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. Automatic differentiation in machine learning: A survey. J. Mach. Learn. Res., 18(1):5595\u20135637, January 2017. [16] Dougal Maclaurin. Modeling, Inference and Optimization\""}, {"x": -0.21751338243484497, "y": -11.805695533752441, "text": "[Page no. 11] \"with Composable Differentiable Procedures. PhD thesis, Harvard University, April 2016. [17] Matthew Johnson et. al. Jax. https://github.com/google/jax, 2018. [18] Mike Innes et. al. Flux.jl. https://github.com/FluxML/Flux.jl, 2018. [19] Eric Jones, Travis Oliphant, Pearu Peterson, et al. SciPy: Open source scienti\ufb01c tools for Python, 2001\u2013. http://www.scipy.org/. [20] Wes McKinney. Data structures for statistical computing in python. In Proceedings of the 9th Python in Science Conference, 51-56, 2010. [21] Pierre Sermanet, Koray Kavukcuoglu, and Yann LeCun. Eblearn: Open-source energy-based learning in c++. In 2009 21st IEEE International Conference on Tools with Arti\ufb01cial Intelligence, pages 693\u2013697. IEEE, 2009. 10  [22] Sharan Chetlur, Cliff Woolley, Philippe Vandermersch, Jonathan D. Cohen, John Tran, Bryan Catanzaro, and Evan Shelhamer. cudnn: Ef\ufb01cient primitives for deep learning. CoRR, abs/1410.0759, 2014. [23] Andrew Lavin. maxdnn: An ef\ufb01cient convolution kernel for deep learning with maxwell gpus, January 2015. [24] Andrew Lavin and Scott Gray. Fast algorithms for convolutional neural networks.\""}, {"x": 3.3907299041748047, "y": -8.259965896606445, "text": "[Page no. 11] \"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4013\u20134021, 2016. [25] Ronan Collobert, Koray Kavukcuoglu, and Cl\u00e9ment Farabet. Torch7: A matlab-like environment for machine learning. In NIPS 2011, 2011. [26] Richard Gabriel. The rise of worse is better. http://dreamsongs.com/RiseOfWorseIsBetter.html. [27] Yann LeCun and Corinna Cortes. MNIST handwritten digit database. http://yann.lecun.com/exdb/mnist/. [28] Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich K\u00fcttler, John Agapiou, Julian Schrittwieser, John Quan, Stephen Gaffney, Stig Petersen, Karen Simonyan, Tom Schaul, Hado van Hasselt, David Silver, Timothy P. Lillicrap, Kevin Calderone, Paul Keet, Anthony Brunasso, David Lawrence, Anders Ekermo, Jacob Repp, and Rodney Tsing. Starcraft II: A new challenge for reinforcement learning. CoRR, abs/1708.04782, 2017. [29] DMLC. Dlpack: Open in memory tensor structure. https://github.com/dmlc/dlpack. [30] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.\""}, {"x": -2.2676169872283936, "y": -10.93483829498291, "text": "[Page no. 11] \"Automatic differentiation in pytorch. In NIPS Workshop, 2017. [31] Dan Piponi. Automatic differentiation, C++ templates, and photogrammetry. J. Graphics, GPU, & Game Tools, 9(4):41\u201355, 2004. [32] Holger Leuck and Hans-Hellmut Nagel. Automatic differentiation facilitates of-integration into steering-angle-based road vehicle tracking. In 1999 Conference on Computer Vision and Pattern Recognition (CVPR \u201999), 23-25 June 1999, Ft. Collins, CO, USA, pages 2360\u20132365, 1999. [33] The Python team. The cpython global interpreter lock. https://wiki.python.org/moin/GlobalInterpreterLock. [34] Giovanni Petrantoni and J\u00f6rg Wollenschl\u00e4ger. Nimtorch. https://github.com/fragcolor- xyz/nimtorch. [35] Austin Huang, Junji Hashimoto, and Sam Stites. Hasktorch. https://github.com/hasktorch/hasktorch. [36] G. Synnaeve, Z. Lin, J. Gehring, D. Gant, V. Mella, V. Khalidov, N. Carion, and N. Usunier. Forward modeling for partial observation strategy games - a starcraft defogger. In Advances in Neural Information Processing Systems, pages 10761\u201310771, 2018. [37] The PyTorch team. Torch Script. https://pytorch.org/docs/stable/jit.html. [38] Justin Luitjens. Cuda streams. GPU technology conference, 2014. [39] Emery D. Berger, Kathryn\""}, {"x": -0.5007907152175903, "y": -2.3814713954925537, "text": "[Page no. 12] \"S. McKinley, Robert D. Blumofe, and Paul R. Wilson. Hoard: A scalable memory allocator for multithreaded applications. In Proceedings of the Ninth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS IX, pages 117\u2013128, New York, NY, USA, 2000. ACM. [40] J. Evans. A scalable concurrent malloc(3) implementation for freebsd. In In BSDCan \u2014 The Technical BSD Conference, May 2006. [41] S. Ghemawat and P. Menage. Tcmalloc: Thread-caching malloc. 11  [42] Benjamin Recht, Christopher R\u00e9, Stephen J. Wright, and Feng Niu. Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems 2011. Proceedings of a meeting held 12-14 December 2011, Granada, Spain., pages 693\u2013701, 2011. [43] Matthew Hertz and Emery D. Berger. Quantifying the performance of garbage collection vs. explicit memory management. In Proceedings of the 20th Annual ACM SIGPLAN Conference\""}, {"x": -2.796785593032837, "y": -10.264120101928711, "text": "[Page no. 12] \"on Object-oriented Programming, Systems, Languages, and Applications, OOPSLA \u201905, pages 313\u2013326, New York, NY, USA, 2005. ACM. [44] The PyTorch team. Pytorch Autograd Pro\ufb01ler. https://pytorch.org/docs/1.0.1/autograd.html#pro\ufb01ler. 12\""}, {"x": -0.1512642502784729, "y": 5.00016450881958, "text": "[Page no. 1] \"Efficient Memory Management for Large Language Model Serving with PagedAttention Woosuk Kwon1,\u2217Zhuohan Li1,\u2217Siyuan Zhuang1 Ying Sheng1,2 Lianmin Zheng1 Cody Hao Yu3 Joseph E. Gonzalez1 Hao Zhang4 Ion Stoica1 1UC Berkeley 2Stanford University 3Independent Researcher 4UC San Diego Abstract High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. How- ever, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention al- gorithm inspired by the classical virtual memory and pag- ing techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to\""}, {"x": 1.788337230682373, "y": 5.130571365356445, "text": "[Page no. 1] \"further reduce mem- ory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4\u00d7 with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM\u2019s source code is publicly available at https://github.com/vllm-project/vllm. 1 Introduction The emergence of large language models (LLMs) like GPT [5, 37] and PaLM [9] have enabled new applications such as pro- gramming assistants [6, 18] and universal chatbots [19, 35] that are starting to profoundly impact our work and daily routines. Many cloud companies [34, 44] are racing to pro- vide these applications as hosted services. However, running these applications is very expensive, requiring a large num- ber of hardware accelerators such as GPUs. According to recent estimates, processing an LLM request can be 10\u00d7 more expensive than a traditional keyword\""}, {"x": -4.046505928039551, "y": -0.18928875029087067, "text": "[Page no. 1] \"query [43]. Given these high costs, increasing the throughput\u2014and hence reducing Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third- party components of this work must be honored. For all other uses, contact the owner/author(s). SOSP \u201923, October 23\u201326, 2023, Koblenz, Germany \u00a9 2023 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-0229-7/23/10. https://doi.org/10.1145/3600006.3613165 NVIDIA A100 40GB Parameters (26GB, 65%) KV Cache (>30%) Others 20 30 40 Memory usage (GB) Parameter size Existing systems vLLM 0 10 20 30 40 Batch size (# requests) 0 0.4k 0.8k 1.2k Throughput (token/s) Figure 1. Left: Memory layout when serving an LLM with 13B parameters on NVIDIA A100. The parameters\""}, {"x": -1.2512457370758057, "y": 2.9166486263275146, "text": "[Page no. 1] \"(gray) persist in GPU memory throughout serving. The memory for the KV cache (red) is (de)allocated per serving request. A small amount of memory (yellow) is used ephemerally for activation. Right: vLLM smooths out the rapid growth curve of KV cache memory seen in existing systems [31, 60], leading to a notable boost in serving throughput. the cost per request\u2014of LLM serving systems is becoming more important. At the core of LLMs lies an autoregressive Transformer model [53]. This model generates words (tokens), one at a time, based on the input (prompt) and the previous sequence of the output\u2019s tokens it has generated so far. For each re- quest, this expensive process is repeated until the model out- puts a termination token. This sequential generation process makes the workload memory-bound, underutilizing the com- putation power of GPUs and limiting the serving throughput. Improving the throughput is possible by batching multi-\""}, {"x": -1.307475209236145, "y": 2.073108434677124, "text": "[Page no. 2] \"ple requests together. However, to process many requests in a batch, the memory space for each request should be efficiently managed. For example, Fig. 1 (left) illustrates the memory distribution for a 13B-parameter LLM on an NVIDIA A100 GPU with 40GB RAM. Approximately 65% of the mem- ory is allocated for the model weights, which remain static during serving. Close to 30% of the memory is used to store the dynamic states of the requests. For Transformers, these states consist of the key and value tensors associated with the attention mechanism, commonly referred to as KV cache [41], which represent the context from earlier tokens to gener- ate new output tokens in sequence. The remaining small \u2217Equal contribution. 1 arXiv:2309.06180v1 [cs.LG] 12 Sep 2023  Orca (Max) Orca (Pow2) Orca (Oracle) vLLM 0 20 40 60 80 100 KV cache usage (%) 20.4 13.3 57.3 8.9 26.8 17.9 13.6 41.6\""}, {"x": -2.1127209663391113, "y": 1.936509609222412, "text": "[Page no. 2] \"38.2 25.2 36.6 96.3 T oken states Reservation Internal frag. External frag. & Others Figure 2. Average percentage of memory wastes in different LLM serving systems during the experiment in \u00a76.2. percentage of memory is used for other data, including ac- tivations \u2013 the ephemeral tensors created when evaluating the LLM. Since the model weights are constant and the ac- tivations only occupy a small fraction of the GPU memory, the way the KV cache is managed is critical in determining the maximum batch size. When managed inefficiently, the KV cache memory can significantly limit the batch size and consequently the throughput of the LLM, as illustrated in Fig. 1 (right). In this paper, we observe that existing LLM serving sys- tems [31, 60] fall short of managing the KV cache memory efficiently. This is mainly because they store the KV cache of a request in contiguous memory space, as\""}, {"x": -2.2820544242858887, "y": -0.13978099822998047, "text": "[Page no. 2] \"most deep learning frameworks [33, 39] require tensors to be stored in contigu- ous memory. However, unlike the tensors in the traditional deep learning workloads, the KV cache has unique charac- teristics: it dynamically grows and shrinks over time as the model generates new tokens, and its lifetime and length are not known a priori. These characteristics make the existing systems\u2019 approach significantly inefficient in two ways: First, the existing systems [31, 60] suffer from internal and external memory fragmentation. To store the KV cache of a request in contiguous space, they pre-allocate a contigu- ous chunk of memory with the request\u2019s maximum length (e.g., 2048 tokens). This can result in severe internal frag- mentation, since the request\u2019s actual length can be much shorter than its maximum length (e.g., Fig. 11). Moreover, even if the actual length is known a priori, the pre-allocation is still inefficient: As the entire chunk\""}, {"x": -1.7107473611831665, "y": 4.000827789306641, "text": "[Page no. 2] \"is reserved during the request\u2019s lifetime, other shorter requests cannot utilize any part of the chunk that is currently unused. Besides, external memory fragmentation can also be significant, since the pre- allocated size can be different for each request. Indeed, our profiling results in Fig. 2 show that only 20.4% - 38.2% of the KV cache memory is used to store the actual token states in the existing systems. Second, the existing systems cannot exploit the opportu- nities for memory sharing. LLM services often use advanced decoding algorithms, such as parallel sampling and beam search, that generate multiple outputs per request. In these scenarios, the request consists of multiple sequences that can partially share their KV cache. However, memory sharing is not possible in the existing systems because the KV cache of the sequences is stored in separate contiguous spaces. To address the above limitations, we propose PagedAt- tention, an\""}, {"x": -0.857569694519043, "y": 6.347646236419678, "text": "[Page no. 2] \"attention algorithm inspired by the operating system\u2019s (OS) solution to memory fragmentation and shar- ing: virtual memory with paging. PagedAttention divides the request\u2019s KV cache into blocks, each of which can contain the attention keys and values of a fixed number of tokens. In PagedAttention, the blocks for the KV cache are not neces- sarily stored in contiguous space. Therefore, we can manage the KV cache in a more flexible way as in OS\u2019s virtual mem- ory: one can think of blocks as pages, tokens as bytes, and requests as processes. This design alleviates internal frag- mentation by using relatively small blocks and allocating them on demand. Moreover, it eliminates external fragmen- tation as all blocks have the same size. Finally, it enables memory sharing at the granularity of a block, across the different sequences associated with the same request or even across the different requests. In this work, we\""}, {"x": -0.1288911998271942, "y": 4.223557472229004, "text": "[Page no. 2] \"build vLLM, a high-throughput distributed LLM serving engine on top of PagedAttention that achieves near-zero waste in KV cache memory. vLLM uses block-level memory management and preemptive request scheduling that are co-designed with PagedAttention. vLLM supports popular LLMs such as GPT [5], OPT [62], and LLaMA [52] with varying sizes, including the ones exceeding the memory capacity of a single GPU. Our evaluations on various models and workloads show that vLLM improves the LLM serving throughput by 2-4\u00d7 compared to the state-of-the-art sys- tems [31, 60], without affecting the model accuracy at all. The improvements are more pronounced with longer sequences, larger models, and more complex decoding algorithms (\u00a74.3). In summary, we make the following contributions: \u2022 We identify the challenges in memory allocation in serving LLMs and quantify their impact on serving performance. \u2022 We propose PagedAttention, an attention algorithm that operates on KV cache stored in non-contiguous paged\""}, {"x": 1.033013105392456, "y": 5.576818943023682, "text": "[Page no. 3] \"memory, which is inspired by the virtual memory and paging in OS. \u2022 We design and implement vLLM, a distributed LLM serving engine built on top of PagedAttention. \u2022 We evaluate vLLM on various scenarios and demonstrate that it substantially outperforms the previous state-of-the- art solutions such as FasterTransformer [31] and Orca [60]. 2 Background In this section, we describe the generation and serving pro- cedures of typical LLMs and the iteration-level scheduling used in LLM serving. 2  2.1 Transformer-Based Large Language Models The task of language modeling is to model the probability of a list of tokens (\ud835\udc651, . . . ,\ud835\udc65\ud835\udc5b). Since language has a natural sequential ordering, it is common to factorize the joint prob- ability over the whole sequence as the product of conditional probabilities (a.k.a. autoregressive decomposition [3]): \ud835\udc43(\ud835\udc65) = \ud835\udc43(\ud835\udc651) \u00b7 \ud835\udc43(\ud835\udc652 | \ud835\udc651) \u00b7 \u00b7 \u00b7 \ud835\udc43(\ud835\udc65\ud835\udc5b| \ud835\udc651, . . . ,\ud835\udc65\ud835\udc5b\u22121).\""}, {"x": 1.519464373588562, "y": 8.345497131347656, "text": "[Page no. 3] \"(1) Transformers [53] have become the de facto standard ar- chitecture for modeling the probability above at a large scale. The most important component of a Transformer-based lan- guage model is its self-attention layers. For an input hidden state sequence (\ud835\udc651, . . . ,\ud835\udc65\ud835\udc5b) \u2208R\ud835\udc5b\u00d7\ud835\udc51, a self-attention layer first applies linear transformations on each position \ud835\udc56to get the query, key, and value vectors: \ud835\udc5e\ud835\udc56= \ud835\udc4a\ud835\udc5e\ud835\udc65\ud835\udc56, \ud835\udc58\ud835\udc56= \ud835\udc4a\ud835\udc58\ud835\udc65\ud835\udc56, \ud835\udc63\ud835\udc56= \ud835\udc4a\ud835\udc63\ud835\udc65\ud835\udc56. (2) Then, the self-attention layer computes the attention score \ud835\udc4e\ud835\udc56\ud835\udc57by multiplying the query vector at one position with all the key vectors before it and compute the output \ud835\udc5c\ud835\udc56as the weighted average over the value vectors: \ud835\udc4e\ud835\udc56\ud835\udc57= exp(\ud835\udc5e\u22a4 \ud835\udc56\ud835\udc58\ud835\udc57/ \u221a \ud835\udc51) \u00cd\ud835\udc56 \ud835\udc61=1 exp(\ud835\udc5e\u22a4 \ud835\udc56\ud835\udc58\ud835\udc61/ \u221a \ud835\udc51) , \ud835\udc5c\ud835\udc56= \ud835\udc56 \u2211\ufe01 \ud835\udc57=1 \ud835\udc4e\ud835\udc56\ud835\udc57\ud835\udc63\ud835\udc57. (3) Besides the computation in Eq. 4, all other components in the Transformer model, including the embedding layer, feed-forward layer, layer normalization [2], residual connec- tion [22],\""}, {"x": 0.0655209943652153, "y": 10.856731414794922, "text": "[Page no. 3] \"output logit computation, and the query, key, and value transformation in Eq. 2, are all applied independently position-wise in a form of \ud835\udc66\ud835\udc56= \ud835\udc53(\ud835\udc65\ud835\udc56). 2.2 LLM Service & Autoregressive Generation Once trained, LLMs are often deployed as a conditional gen- eration service (e.g., completion API [34] or chatbot [19, 35]). A request to an LLM service provides a list of input prompt tokens (\ud835\udc651, . . . ,\ud835\udc65\ud835\udc5b), and the LLM service generates a list of output tokens (\ud835\udc65\ud835\udc5b+1, . . . ,\ud835\udc65\ud835\udc5b+\ud835\udc47) according to Eq. 1. We refer to the concatenation of the prompt and output lists as sequence. Due to the decomposition in Eq. 1, the LLM can only sam- ple and generate new tokens one by one, and the generation process of each new token depends on all the previous tokens in that sequence, specifically their key and value vectors. In this sequential generation process, the key\""}, {"x": -0.11949022859334946, "y": 10.764484405517578, "text": "[Page no. 3] \"and value vectors of existing tokens are often cached for generating future tokens, known as KV cache. Note that the KV cache of one token depends on all its previous tokens. This means that the KV cache of the same token appearing at different positions in a sequence will be different. Given a request prompt, the generation computation in the LLM service can be decomposed into two phases: The prompt phase takes the whole user prompt (\ud835\udc651, . . . ,\ud835\udc65\ud835\udc5b) as input and computes the probability of the first new to- ken \ud835\udc43(\ud835\udc65\ud835\udc5b+1 | \ud835\udc651, . . . ,\ud835\udc65\ud835\udc5b). During this process, also gener- ates the key vectors \ud835\udc581, . . . ,\ud835\udc58\ud835\udc5band value vectors \ud835\udc631, . . . , \ud835\udc63\ud835\udc5b. Since prompt tokens \ud835\udc651, . . . ,\ud835\udc65\ud835\udc5bare all known, the computa- tion of the prompt phase can be parallelized using matrix- matrix multiplication operations. Therefore, this phase\""}, {"x": -4.9508562088012695, "y": 1.2411545515060425, "text": "[Page no. 3] \"can efficiently use the parallelism inherent in GPUs. The autoregressive generation phase generates the re- maining new tokens sequentially. At iteration \ud835\udc61, the model takes one token \ud835\udc65\ud835\udc5b+\ud835\udc61as input and computes the probability \ud835\udc43(\ud835\udc65\ud835\udc5b+\ud835\udc61+1 | \ud835\udc651, . . . ,\ud835\udc65\ud835\udc5b+\ud835\udc61) with the key vectors \ud835\udc581, . . . ,\ud835\udc58\ud835\udc5b+\ud835\udc61and value vectors \ud835\udc631, . . . , \ud835\udc63\ud835\udc5b+\ud835\udc61. Note that the key and value vectors at positions 1 to \ud835\udc5b+ \ud835\udc61\u22121 are cached at previous iterations, only the new key and value vector \ud835\udc58\ud835\udc5b+\ud835\udc61and \ud835\udc63\ud835\udc5b+\ud835\udc61are com- puted at this iteration. This phase completes either when the sequence reaches a maximum length (specified by users or limited by LLMs) or when an end-of-sequence (<eos>) token is emitted. The computation at different iterations cannot be parallelized due to the data dependency and often uses matrix-vector multiplication, which is less efficient. As a re- sult, this phase severely underutilizes GPU computation and becomes memory-bound,\""}, {"x": 0.6125156879425049, "y": 1.8236297369003296, "text": "[Page no. 3] \"being responsible for most portion of the latency of a single request. 2.3 Batching Techniques for LLMs The compute utilization in serving LLMs can be improved by batching multiple requests. Because the requests share the same model weights, the overhead of moving weights is amortized across the requests in a batch, and can be over- whelmed by the computational overhead when the batch size is sufficiently large. However, batching the requests to an LLM service is non-trivial for two reasons. First, the requests may arrive at different times. A naive batching strat- egy would either make earlier requests wait for later ones or delay the incoming requests until earlier ones finish, lead- ing to significant queueing delays. Second, the requests may have vastly different input and output lengths (Fig. 11). A straightforward batching technique would pad the inputs and outputs of the requests to equalize their lengths, wasting GPU computation\""}, {"x": 0.5325531363487244, "y": 2.0832231044769287, "text": "[Page no. 4] \"and memory. To address this problem, fine-grained batching mecha- nisms, such as cellular batching [16] and iteration-level sched- uling [60], have been proposed. Unlike traditional methods that work at the request level, these techniques operate at the iteration level. After each iteration, completed requests are removed from the batch, and new ones are added. There- fore, a new request can be processed after waiting for a single iteration, not waiting for the entire batch to complete. Moreover, with special GPU kernels, these techniques elim- inate the need to pad the inputs and outputs. By reducing the queueing delay and the inefficiencies from padding, the fine-grained batching mechanisms significantly increase the throughput of LLM serving. 3  Four score and seven years ago our fathers brought forth <eos> <resv> \u2026 <resv> You only live once <eos> <resv> \u2026 <resv> 2038 slots never used (internal fragmentation) 2 slots future used (reserved) External\""}, {"x": -2.032198429107666, "y": 2.9546849727630615, "text": "[Page no. 4] \"fragmentation 7 KV cache states for request A\u2019s prompt 3 KV cache states for request B\u2019s prompt 1 slot future used (reserved) 507 slots never used (Internal fragmentation) Request B current iteration Request A current iteration 1 slot for generated token Figure 3. KV cache memory management in existing systems. Three types of memory wastes \u2013 reserved, internal fragmentation, and external fragmentation \u2013 exist that prevent other requests from fitting into the memory. The token in each memory slot represents its KV cache. Note the same tokens can have different KV cache when at different positions. 3 Memory Challenges in LLM Serving Although fine-grained batching reduces the waste of com- puting and enables requests to be batched in a more flexible way, the number of requests that can be batched together is still constrained by GPU memory capacity, particularly the space allocated to store the KV cache. In other words,\""}, {"x": -1.9588322639465332, "y": 1.2342805862426758, "text": "[Page no. 4] \"the serving system\u2019s throughput is memory-bound. Overcom- ing this memory-bound requires addressing the following challenges in the memory management: Large KV cache. The KV Cache size grows quickly with the number of requests. As an example, for the 13B parameter OPT model [62], the KV cache of a single token demands 800 KB of space, calculated as 2 (key and value vectors) \u00d7 5120 (hidden state size) \u00d7 40 (number of layers) \u00d7 2 (bytes per FP16). Since OPT can generate sequences up to 2048 tokens, the memory required to store the KV cache of one request can be as much as 1.6 GB. Concurrent GPUs have memory capacities in the tens of GBs. Even if all available memory was allocated to KV cache, only a few tens of requests could be accommodated. Moreover, inefficient memory manage- ment can further decrease the batch size, as shown in Fig. 2. Additionally,\""}, {"x": -2.929419755935669, "y": 2.5864789485931396, "text": "[Page no. 4] \"given the current trends, the GPU\u2019s computa- tion speed grows faster than the memory capacity [17]. For example, from NVIDIA A100 to H100, The FLOPS increases by more than 2x, but the GPU memory stays at 80GB max- imum. Therefore, we believe the memory will become an increasingly significant bottleneck. Complex decoding algorithms. LLM services offer a range of decoding algorithms for users to select from, each with varying implications for memory management complexity. For example, when users request multiple random samples from a single input prompt, a typical use case in program suggestion [18], the KV cache of the prompt part, which accounts for 12% of the total KV cache memory in our ex- periment (\u00a76.3), can be shared to minimize memory usage. On the other hand, the KV cache during the autoregressive generation phase should remain unshared due to the dif- ferent sample results and their dependence on\""}, {"x": -2.702451229095459, "y": 2.876208782196045, "text": "[Page no. 4] \"context and position. The extent of KV cache sharing depends on the specific decoding algorithm employed. In more sophisticated algorithms like beam search [49], different request beams can share larger portions (up to 55% memory saving, see \u00a76.3) of their KV cache, and the sharing pattern evolves as the decoding process advances. Scheduling for unknown input & output lengths. The requests to an LLM service exhibit variability in their input and output lengths. This requires the memory management system to accommodate a wide range of prompt lengths. In addition, as the output length of a request grows at decoding, the memory required for its KV cache also expands and may exhaust available memory for incoming requests or ongoing generation for existing prompts. The system needs to make scheduling decisions, such as deleting or swapping out the KV cache of some requests from GPU memory. 3.1 Memory Management in Existing Systems\""}, {"x": -2.299793243408203, "y": 0.04005790874361992, "text": "[Page no. 4] \"Since most operators in current deep learning frameworks [33, 39] require tensors to be stored in contiguous memory, previous LLM serving systems [31, 60] also store the KV cache of one request as a contiguous tensor across the differ- ent positions. Due to the unpredictable output lengths from the LLM, they statically allocate a chunk of memory for a request based on the request\u2019s maximum possible sequence length, irrespective of the actual input or eventual output length of the request. Fig. 3 illustrates two requests: request A with 2048 max- imum possible sequence length and request B with a max- imum of 512. The chunk pre-allocation scheme in existing systems has three primary sources of memory wastes: re- served slots for future tokens, internal fragmentation due to over-provisioning for potential maximum sequence lengths, and external fragmentation from the memory allocator like the buddy allocator. The external fragmentation will never be\""}, {"x": -3.397007465362549, "y": 4.012348175048828, "text": "[Page no. 5] \"used for generated tokens, which is known before serving a request. Internal fragmentation also remains unused, but this is only realized after a request has finished sampling. They are both pure memory waste. Although the reserved memory is eventually used, reserving this space for the en- tire request\u2019s duration, especially when the reserved space is large, occupies the space that could otherwise be used to process other requests. We visualize the average percentage of memory wastes in our experiments in Fig. 2, revealing that the actual effective memory in previous systems can be as low as 20.4%. 4  KV Cache Manager Scheduler CPU Block Allocator GPU Block Allocator Block tables Worker 0 Model Shard 0 Cache Engine Worker 1 Model Shard 1 Cache Engine Worker N - 1 Model Shard N - 1 Cache Engine \u2026 Figure 4. vLLM system overview. Although compaction [54] has been proposed as a\""}, {"x": -0.6859509348869324, "y": 4.151002407073975, "text": "[Page no. 5] \"poten- tial solution to fragmentation, performing compaction in a performance-sensitive LLM serving system is impractical due to the massive KV cache. Even with compaction, the pre-allocated chunk space for each request prevents memory sharing specific to decoding algorithms in existing memory management systems. 4 Method In this work, we develop a new attention algorithm, Page- dAttention, and build an LLM serving engine, vLLM, to tackle the challenges outlined in \u00a73. The architecture of vLLM is shown in Fig. 4. vLLM adopts a centralized scheduler to coordinate the execution of distributed GPU workers. The KV cache manager effectively manages the KV cache in a paged fashion, enabled by PagedAttention. Specifically, the KV cache manager manages the physical KV cache memory on the GPU workers through the instructions sent by the centralized scheduler. Next, We describe the PagedAttention algorithm in \u00a74.1. With that, we show the design of the KV cache manager\""}, {"x": -0.32638370990753174, "y": 6.189406394958496, "text": "[Page no. 5] \"in \u00a74.2 and how it facilitates PagedAttention in \u00a74.3, respec- tively. Then, we show how this design facilitates effective memory management for various decoding methods (\u00a74.4) and handles the variable length input and output sequences (\u00a74.5). Finally, we show how the system design of vLLM works in a distributed setting (\u00a74.6). 4.1 PagedAttention To address the memory challenges in \u00a73, we introduce Page- dAttention, an attention algorithm inspired by the classic idea of paging [25] in operating systems. Unlike the traditional attention algorithms, PagedAttention allows storing continu- ous keys and values in non-contiguous memory space. Specif- ically, PagedAttention partitions the KV cache of each se- quence into KV blocks. Each block contains the key and value vectors for a fixed number of tokens,1 which we denote as KV 1In Transformer, each token has a set of key and value vectors across layers and attention heads within a layer. All the\""}, {"x": 0.4548346698284149, "y": 7.840874195098877, "text": "[Page no. 5] \"key and value vectors can be managed together within a single KV block, or the key and value vectors at different heads and layers can each have a separate block and be managed in separate block tables. The two designs have no performance difference and we choose the second one for easy implementation. forth Query vector years ago our fathers brought forth Four score and seven Key and value vectors Block 1 Block 2 Block 0 Figure 5. Illustration of the PagedAttention algorithm, where the attention key and values vectors are stored as non-contiguous blocks in the memory. block size (\ud835\udc35). Denote the key block \ud835\udc3e\ud835\udc57= (\ud835\udc58(\ud835\udc57\u22121)\ud835\udc35+1, . . . ,\ud835\udc58\ud835\udc57\ud835\udc35) and value block \ud835\udc49\ud835\udc57= (\ud835\udc63(\ud835\udc57\u22121)\ud835\udc35+1, . . . , \ud835\udc63\ud835\udc57\ud835\udc35). The attention com- putation in Eq. 4 can be transformed into the following block- wise computation: \ud835\udc34\ud835\udc56\ud835\udc57= exp(\ud835\udc5e\u22a4 \ud835\udc56\ud835\udc3e\ud835\udc57/ \u221a \ud835\udc51) \u00cd\u2308\ud835\udc56/\ud835\udc35\u2309 \ud835\udc61=1 exp(\ud835\udc5e\u22a4 \ud835\udc56\ud835\udc3e\ud835\udc611/ \u221a \ud835\udc51) , \ud835\udc5c\ud835\udc56=\""}, {"x": -0.1529623121023178, "y": 7.105190753936768, "text": "[Page no. 5] \"\u2308\ud835\udc56/\ud835\udc35\u2309 \u2211\ufe01 \ud835\udc57=1 \ud835\udc49\ud835\udc57\ud835\udc34\u22a4 \ud835\udc56\ud835\udc57, (4) where \ud835\udc34\ud835\udc56\ud835\udc57= (\ud835\udc4e\ud835\udc56,(\ud835\udc57\u22121)\ud835\udc35+1, . . . ,\ud835\udc4e\ud835\udc56,\ud835\udc57\ud835\udc35) is the row vector of atten- tion score on \ud835\udc57-th KV block. During the attention computation, the PagedAttention kernel identifies and fetches different KV blocks separately. We show an example of PagedAttention in Fig. 5: The key and value vectors are spread across three blocks, and the three blocks are not contiguous on the physical memory. At each time, the kernel multiplies the query vector \ud835\udc5e\ud835\udc56of the query token (\u201cforth\u201d) and the key vectors \ud835\udc3e\ud835\udc57in a block (e.g., key vectors of \u201cFour score and seven\u201d for block 0) to compute the attention score\ud835\udc34\ud835\udc56\ud835\udc57, and later multiplies\ud835\udc34\ud835\udc56\ud835\udc57with the value vectors \ud835\udc49\ud835\udc57in a block to derive the final attention output \ud835\udc5c\ud835\udc56. In summary, the PagedAttention algorithm allows the KV blocks to be stored in non-contiguous physical memory, which enables more flexible paged memory management in vLLM. 4.2 KV Cache\""}, {"x": -2.3999154567718506, "y": 6.028212547302246, "text": "[Page no. 5] \"Manager The key idea behind vLLM\u2019s memory manager is analogous to the virtual memory [25] in operating systems. OS parti- tions memory into fixed-sized pages and maps user programs\u2019 logical pages to physical pages. Contiguous logical pages can correspond to non-contiguous physical memory pages, al- lowing user programs to access memory as though it were contiguous. Moreover, physical memory space needs not to be fully reserved in advance, enabling the OS to dynamically allocate physical pages as needed. vLLM uses the ideas be- hind virtual memory to manage the KV cache in an LLM service. Enabled by PagedAttention, we organize the KV cache as fixed-size KV blocks, like pages in virtual memory. A request\u2019s KV cache is represented as a series of logical KV blocks, filled from left to right as new tokens and their KV cache are generated. The last KV block\u2019s unfilled positions are reserved for future generations.\""}, {"x": -5.547123432159424, "y": 8.130670547485352, "text": "[Page no. 6] \"On GPU workers, a block engine allocates a contiguous chunk of GPU DRAM and 5  Request A Four score and seven years ago our fathers brought Prompt: \u201cFour score and seven years ago our\u201d Outputs: \u201cfathers\u201d \u2192 \u201cbrought\u201d \u2192 \u2026 Block 0 Block 1 Block 2 Block 3 years ago our fathers brought Four score and seven Physical KV blocks (on GPU DRAM) Block 0 Block 1 Block 2 Block 3 Block 4 Block 5 Block 6 Block 7 Block 8 Logical KV blocks Physical block number # filled 7 4 1 3 \u2192 4 3 1 \u2013 \u2013 Block Table 1 1 1 1 1 1 1 2 3 1 1 1 1 2 3 3 3 1 1 1 1 3 1 1 1 2 1 1 1 1 3 Figure 6. Block table translation in vLLM. divides it into physical KV blocks (this is also done\""}, {"x": -3.0537805557250977, "y": 6.712828636169434, "text": "[Page no. 6] \"on CPU RAM for swapping; see \u00a74.5). The KV block manager also maintains block tables\u2014the mapping between logical and physical KV blocks of each request. Each block table entry records the corresponding physical blocks of a logical block and the number of filled positions. Separating logical and physical KV blocks allows vLLM to dynamically grow the KV cache memory without reserving it for all positions in advance, which eliminates most memory waste in existing systems, as in Fig. 2. 4.3 Decoding with PagedAttention and vLLM Next, we walk through an example, as in Fig. 6, to demon- strate how vLLM executes PagedAttention and manages the memory during the decoding process of a single input se- quence: 1 \u25cbAs in OS\u2019s virtual memory, vLLM does not require reserving the memory for the maximum possible generated sequence length initially. Instead, it reserves only the nec- essary KV blocks to accommodate the KV\""}, {"x": -3.4882566928863525, "y": 8.196212768554688, "text": "[Page no. 6] \"cache generated during prompt computation. In this case, The prompt has 7 tokens, so vLLM maps the first 2 logical KV blocks (0 and 1) to 2 physical KV blocks (7 and 1, respectively). In the prefill step, vLLM generates the KV cache of the prompts and the first output token with a conventional self-attention algorithm (e.g., [13]). vLLM then stores the KV cache of the first 4 tokens in logical block 0 and the following 3 tokens in logical block 1. The remaining slot is reserved for the subsequent autoregressive generation phase. 2 \u25cbIn the first autoregressive decoding step, vLLM generates the new token with the PagedAttention algorithm on physical blocks 7 and 1. Since one slot remains available in the last logical block, the newly generated KV cache is stored there, and the block table\u2019s #filled record is updated. 3 \u25cbAt the second decoding step, as the last\""}, {"x": -4.362478256225586, "y": 8.53790283203125, "text": "[Page no. 6] \"logical block is full, vLLM stores the newly generated KV cache in a new logical block; vLLM allocates a new physical block (physical block 3) for it and stores this mapping in the block table. Globally, for each decoding iteration, vLLM first selects a set of candidate sequences for batching (more in \u00a74.5), and allocates the physical blocks for the newly required logical blocks. Then, vLLM concatenates all the input tokens of the current iteration (i.e., all tokens for prompt phase Four score and seven years ago our fathers brought Block 0 Block 1 Block 2 Block 3 years ago our fathers of times brought It was the best Four score and seven Physical KV blocks Block 0 Block 1 Block 2 Block 3 Block 4 Block 5 Block 6 Block 7 Block 8 Logical KV blocks It was the best of times Block 0 Block 1 Block 2 Logical\""}, {"x": -2.5777032375335693, "y": 6.1004743576049805, "text": "[Page no. 6] \"KV blocks Request A Request B Figure 7. Storing the KV cache of two requests at the same time in vLLM. requests and the latest tokens for generation phase requests) as one sequence and feeds it into the LLM. During LLM\u2019s computation, vLLM uses the PagedAttention kernel to access the previous KV cache stored in the form of logical KV blocks and saves the newly generated KV cache into the physical KV blocks. Storing multiple tokens within a KV block (block size > 1) enables the PagedAttention kernel to process the KV cache across more positions in parallel, thus increasing the hardware utilization and reducing latency. However, a larger block size also increases memory fragmentation. We study the effect of block size in \u00a77.2. Again, vLLM dynamically assigns new physical blocks to logical blocks as more tokens and their KV cache are gener- ated. As all the blocks are filled\""}, {"x": -3.4372642040252686, "y": 5.598181247711182, "text": "[Page no. 6] \"from left to right and a new physical block is only allocated when all previous blocks are full, vLLM limits all the memory wastes for a request within one block, so it can effectively utilize all the memory, as shown in Fig. 2. This allows more requests to fit into mem- ory for batching\u2014hence improving the throughput. Once a request finishes its generation, its KV blocks can be freed to store the KV cache of other requests. In Fig. 7, we show an example of vLLM managing the memory for two sequences. The logical blocks of the two sequences are mapped to differ- ent physical blocks within the space reserved by the block engine in GPU workers. The neighboring logical blocks of both sequences do not need to be contiguous in physical GPU memory and the space of physical blocks can be effectively utilized by both sequences. 4.4 Application to\""}, {"x": -2.0210020542144775, "y": 9.34969425201416, "text": "[Page no. 7] \"Other Decoding Scenarios \u00a74.3 shows how PagedAttention and vLLM handle basic de- coding algorithms, such as greedy decoding and sampling, that take one user prompt as input and generate a single out- put sequence. In many successful LLM applications [18, 34], an LLM service must offer more complex decoding scenarios that exhibit complex accessing patterns and more opportuni- ties for memory sharing. We show the general applicability of vLLM on them in this section. Parallel sampling. In LLM-based program assistants [6, 18], an LLM generates multiple sampled outputs for a single in- put prompt; users can choose a favorite output from various candidates. So far we have implicitly assumed that a request 6  Sample A1 Four score and seven years ago our fathers Block 0 Block 1 years ago our mothers years ago our fathers Four score and seven Physical KV blocks Block 0 Block 1 Block 2 Block\""}, {"x": -2.4534013271331787, "y": 9.474632263183594, "text": "[Page no. 7] \"3 Block 4 Block 5 Block 6 Block 7 Block 8 Logical KV blocks Four score and seven years ago our mothers Block 0 Block 1 Logical KV blocks Sample A2 Copy-on-write Ref count: 2 \u2192 1 Figure 8. Parallel sampling example. generates a single sequence. In the remainder of this paper, we assume the more general case in which a request gener- ates multiple sequences. In parallel sampling, one request includes multiple samples sharing the same input prompt, allowing the KV cache of the prompt to be shared as well. Via its PagedAttention and paged memory management, vLLM can realize this sharing easily and save memory. Fig. 8 shows an example of parallel decoding for two out- puts. Since both outputs share the same prompt, we only reserve space for one copy of the prompt\u2019s state at the prompt phase; the logical blocks for the prompts of both sequences\""}, {"x": -3.857408046722412, "y": 7.456071853637695, "text": "[Page no. 7] \"are mapped to the same physical blocks: the logical block 0 and 1 of both sequences are mapped to physical blocks 7 and 1, respectively. Since a single physical block can be mapped to multiple logical blocks, we introduce a reference count for each physical block. In this case, the reference counts for physical blocks 7 and 1 are both 2. At the generation phase, the two outputs sample different output tokens and need separate storage for KV cache. vLLM implements a copy-on- write mechanism at the block granularity for the physical blocks that need modification by multiple sequences, similar to the copy-on-write technique in OS virtual memory (e.g., when forking a process). Specifically, in Fig. 8, when sample A1 needs to write to its last logical block (logical block 1), vLLM recognizes that the reference count of the correspond- ing physical block (physical block 1) is greater than 1;\""}, {"x": -2.4149222373962402, "y": 8.022122383117676, "text": "[Page no. 7] \"it allocates a new physical block (physical block 3), instructs the block engine to copy the information from physical block 1, and decreases the reference count to 1. Next, when sample A2 writes to physical block 1, the reference count is already reduced to 1; thus A2 directly writes its newly generated KV cache to physical block 1. In summary, vLLM enables the sharing of most of the space used to store the prompts\u2019 KV cache across multiple output samples, with the exception of the final logical block, which is managed by a copy-on-write mechanism. By sharing physical blocks across multiple samples, memory usage can be greatly reduced, especially for long input prompts. Beam search. In LLM tasks like machine translation [59], the users expect the top-\ud835\udc58most appropriate translations out- put by the LLM. Beam search [49] is widely used to decode the most probable output sequence from an LLM,\""}, {"x": -4.54801607131958, "y": 10.32606029510498, "text": "[Page no. 7] \"as it miti- gates the computational complexity of fully traversing the Block 10 Block 11 Block 1 Block 3 Block 6 Block 7 Block 5 Block 0 Block 2 Block 4 Block 8 Block 9 Block 12 Beam candidate 0 Beam candidate 1 Beam candidate 2 Beam candidate 3 Figure 9. Beam search example. sample space. The algorithm relies on the beam width pa- rameter \ud835\udc58, which determines the number of top candidates retained at every step. During decoding, beam search ex- pands each candidate sequence in the beam by considering all possible tokens, computes their respective probabilities us- ing the LLM, and retains the top-\ud835\udc58most probable sequences out of \ud835\udc58\u00b7 |\ud835\udc49| candidates, where |\ud835\udc49| is the vocabulary size. Unlike parallel decoding, beam search facilities sharing not only the initial prompt blocks but also other blocks across different candidates, and the sharing patterns dynamically change as the decoding process advances,\""}, {"x": -4.696597099304199, "y": 9.341489791870117, "text": "[Page no. 7] \"similar to the pro- cess tree in the OS created by compound forks. Fig. 9 shows how vLLM manages the KV blocks for a beam search ex- ample with \ud835\udc58= 4. Prior to the iteration illustrated as the dotted line, each candidate sequence has used 4 full logi- cal blocks. All beam candidates share the first block 0 (i.e., prompt). Candidate 3 digresses from others from the second block. Candidates 0-2 share the first 3 blocks and diverge at the fourth block. At subsequent iterations, the top-4 prob- able candidates all originate from candidates 1 and 2. As the original candidates 0 and 3 are no longer among the top candidates, their logical blocks are freed, and the refer- ence counts of corresponding physical blocks are reduced. vLLM frees all physical blocks whose reference counts reach 0 (blocks 2, 4, 5, 8). Then, vLLM allocates new physical blocks (blocks 9-12)\""}, {"x": -2.363736152648926, "y": 7.87190580368042, "text": "[Page no. 7] \"to store the new KV cache from the new can- didates. Now, all candidates share blocks 0, 1, 3; candidates 0 and 1 share block 6, and candidates 2 and 3 further share block 7. Previous LLM serving systems require frequent memory copies of the KV cache across the beam candidates. For exam- ple, in the case shown in Fig. 9, after the dotted line, candidate 3 would need to copy a large portion of candidate 2\u2019s KV cache to continue generation. This frequent memory copy overhead is significantly reduced by vLLM\u2019s physical block sharing. In vLLM, most blocks of different beam candidates can be shared. The copy-on-write mechanism is applied only when the newly generated tokens are within an old shared block, as in parallel decoding. This involves only copying one block of data. Shared prefix. Commonly, the LLM user provides a (long) description of the task including instructions\""}, {"x": 1.0690319538116455, "y": 10.947383880615234, "text": "[Page no. 8] \"and example inputs and outputs, also known as system prompt [36]. The description is concatenated with the actual task input to form the prompt of the request. The LLM generates outputs based 7  Translate English to French: \u201csea otter\u201d => \u201cloutre de mer\u201d \u201cpeppermint\u201d => \u201cmenthe poivr\u00e9e\u201d \u201cplush girafe\u201d => \u201cgirafe en peluche\u201d \u201ccheese\u201d => \u201cfromage\u201d Translate English to French: \u201csea otter\u201d => \u201cloutre de mer\u201d \u201cpeppermint\u201d => \u201cmenthe poivr\u00e9e\u201d \u201cplush girafe\u201d => \u201cgirafe en peluche\u201d \u201cI love you\u201d => \u201cJe t\u2019amie\u201d Shared prefix Task input Task output Sequence A Prompt Sequence B Prompt Sequence A LLM output Sequence B LLM output Figure 10. Shared prompt example for machine translation. The examples are adopted from [5]. on the full prompt. Fig. 10 shows an example. Moreover, the shared prefix can be further tuned, via prompt engineering, to improve the accuracy of the downstream tasks [26, 27]. For this type\""}, {"x": -1.5725741386413574, "y": 8.59386157989502, "text": "[Page no. 8] \"of application, many user prompts share a prefix, thus the LLM service provider can store the KV cache of the prefix in advance to reduce the redundant computa- tion spent on the prefix. In vLLM, this can be conveniently achieved by reserving a set of physical blocks for a set of predefined shared prefixes by the LLM service provider, as how OS handles shared library across processes. A user in- put prompt with the shared prefix can simply map its logi- cal blocks to the cached physical blocks (with the last block marked copy-on-write). The prompt phase computation only needs to execute on the user\u2019s task input. Mixed decoding methods. The decoding methods dis- cussed earlier exhibit diverse memory sharing and access- ing patterns. Nonetheless, vLLM facilitates the simultane- ous processing of requests with different decoding prefer- ences, which existing systems cannot efficiently do. This is because vLLM conceals the\""}, {"x": -1.817154884338379, "y": 5.087948322296143, "text": "[Page no. 8] \"complex memory sharing be- tween different sequences via a common mapping layer that translates logical blocks to physical blocks. The LLM and its execution kernel only see a list of physical block IDs for each sequence and do not need to handle sharing pat- terns across sequences. Compared to existing systems, this approach broadens the batching opportunities for requests with different sampling requirements, ultimately increasing the system\u2019s overall throughput. 4.5 Scheduling and Preemption When the request traffic surpasses the system\u2019s capacity, vLLM must prioritize a subset of requests. In vLLM, we adopt the first-come-first-serve (FCFS) scheduling policy for all requests, ensuring fairness and preventing starvation. When vLLM needs to preempt requests, it ensures that the earliest arrived requests are served first and the latest requests are preempted first. LLM services face a unique challenge: the input prompts for an LLM can vary significantly in length, and the resulting output lengths\""}, {"x": -5.254654407501221, "y": 6.610934734344482, "text": "[Page no. 8] \"are not known a priori, contingent on both the input prompt and the model. As the number of requests and their outputs grow, vLLM can run out of the GPU\u2019s phys- ical blocks to store the newly generated KV cache. There are two classic questions that vLLM needs to answer in this context: (1) Which blocks should it evict? (2) How to recover evicted blocks if needed again? Typically, eviction policies use heuristics to predict which block will be accessed fur- thest in the future and evict that block. Since in our case we know that all blocks of a sequence are accessed together, we implement an all-or-nothing eviction policy, i.e., either evict all or none of the blocks of a sequence. Furthermore, multi- ple sequences within one request (e.g., beam candidates in one beam search request) are gang-scheduled as a sequence group. The sequences within one sequence group are\""}, {"x": -4.928454875946045, "y": 6.428740978240967, "text": "[Page no. 8] \"always preempted or rescheduled together due to potential memory sharing across those sequences. To answer the second ques- tion of how to recover an evicted block, we consider two techniques: Swapping. This is the classic technique used by most virtual memory implementations which copy the evicted pages to a swap space on the disk. In our case, we copy evicted blocks to the CPU memory. As shown in Fig. 4, besides the GPU block allocator, vLLM includes a CPU block allocator to manage the physical blocks swapped to CPU RAM. When vLLM exhausts free physical blocks for new tokens, it selects a set of sequences to evict and transfer their KV cache to the CPU. Once it preempts a sequence and evicts its blocks, vLLM stops accepting new requests until all preempted sequences are completed. Once a request completes, its blocks are freed from memory, and the blocks of a\""}, {"x": -4.366794586181641, "y": 2.886752128601074, "text": "[Page no. 8] \"preempted sequence are brought back in to continue the processing of that sequence. Note that with this design, the number of blocks swapped to the CPU RAM never exceeds the number of total physical blocks in the GPU RAM, so the swap space on the CPU RAM is bounded by the GPU memory allocated for the KV cache. Recomputation. In this case, we simply recompute the KV cache when the preempted sequences are rescheduled. Note that recomputation latency can be significantly lower than the original latency, as the tokens generated at decoding can be concatenated with the original user prompt as a new prompt\u2014their KV cache at all positions can be generated in one prompt phase iteration. The performances of swapping and recomputation depend on the bandwidth between CPU RAM and GPU memory and the computation power of the GPU. We examine the speeds of swapping and recomputation in \u00a77.3.\""}, {"x": -1.0660138130187988, "y": 0.9655479788780212, "text": "[Page no. 9] \"4.6 Distributed Execution Many LLMs have parameter sizes exceeding the capacity of a single GPU [5, 9]. Therefore, it is necessary to partition them across distributed GPUs and execute them in a model parallel fashion [28, 63]. This calls for a memory manager capable of handling distributed memory. vLLM is effective in distributed settings by supporting the widely used Megatron-LM style tensor model parallelism strategy on Transformers [47]. This strategy adheres to an SPMD (Single Program Multiple Data) execution schedule, wherein the linear layers are partitioned 8  Table 1. Model sizes and server configurations. Model size 13B 66B 175B GPUs A100 4\u00d7A100 8\u00d7A100-80GB Total GPU memory 40 GB 160 GB 640 GB Parameter size 26 GB 132 GB 346 GB Memory for KV cache 12 GB 21 GB 264 GB Max. # KV cache slots 15.7K 9.7K 60.1K to perform block-wise matrix multiplication, and the the GPUs constantly synchronize\""}, {"x": -5.6158061027526855, "y": 4.099750995635986, "text": "[Page no. 9] \"intermediate results via an all- reduce operation. Specifically, the attention operator is split on the attention head dimension, each SPMD process takes care of a subset of attention heads in multi-head attention. We observe that even with model parallel execution, each model shard still processes the same set of input tokens, thus requiring the KV Cache for the same positions. Therefore, vLLM features a single KV cache manager within the cen- tralized scheduler, as in Fig. 4. Different GPU workers share the manager, as well as the mapping from logical blocks to physical blocks. This common mapping allows GPU workers to execute the model with the physical blocks provided by the scheduler for each input request. Although each GPU worker has the same physical block IDs, a worker only stores a portion of the KV cache for its corresponding attention heads. In each step, the scheduler first prepares the message\""}, {"x": -5.699893951416016, "y": 4.089460849761963, "text": "[Page no. 9] \"with input token IDs for each request in the batch, as well as the block table for each request. Next, the scheduler broadcasts this control message to the GPU workers. Then, the GPU workers start to execute the model with the input token IDs. In the attention layers, the GPU workers read the KV cache according to the block table in the control message. During execution, the GPU workers synchronize the intermediate results with the all-reduce communication primitive without the coordination of the scheduler, as in [47]. In the end, the GPU workers send the sampled tokens of this iteration back to the scheduler. In summary, GPU workers do not need to synchronize on memory management as they only need to receive all the memory management information at the beginning of each decoding iteration along with the step inputs. 5 Implementation vLLM is an end-to-end serving system with a FastAPI\""}, {"x": 1.7621084451675415, "y": 0.6707232594490051, "text": "[Page no. 9] \"[15] frontend and a GPU-based inference engine. The frontend extends the OpenAI API [34] interface, allowing users to customize sampling parameters for each request, such as the maximum sequence length and the beam width \ud835\udc58. The vLLM engine is written in 8.5K lines of Python and 2K lines of C++/CUDA code. We develop control-related components in- cluding the scheduler and the block manager in Python while developing custom CUDA kernels for key operations such as PagedAttention. For the model executor, we implement pop- ular LLMs such as GPT [5], OPT [62], and LLaMA [52] using 0 500 1000 1500 2000 # T okens 0.0 0.5 1.0 1.5 2.0 Density 1e\u22122 Input (mean: 161.31) Output (mean: 337.99) (a) ShareGPT 0 500 1000 1500 2000 # T okens 0 2 4 6 8 Density 1e\u22122 Input (mean: 19.31) Output (mean: 58.45) (b) Alpaca Figure 11. Input and output length distributions of the\""}, {"x": 0.28669145703315735, "y": -0.09902781248092651, "text": "[Page no. 9] \"(a) ShareGPT and (b) Alpaca datasets. PyTorch [39] and Transformers [58]. We use NCCL [32] for tensor communication across the distributed GPU workers. 5.1 Kernel-level Optimization Since PagedAttention introduces memory access patterns that are not efficiently supported by existing systems, we develop several GPU kernels for optimizing it. (1) Fused re- shape and block write. In every Transformer layer, the new KV cache are split into blocks, reshaped to a memory layout optimized for block read, then saved at positions specified by the block table. To minimize kernel launch overheads, we fuse them into a single kernel. (2) Fusing block read and atten- tion. We adapt the attention kernel in FasterTransformer [31] to read KV cache according to the block table and perform attention operations on the fly. To ensure coalesced memory access, we assign a GPU warp to read each block. More- over, we add support for variable sequence\""}, {"x": -2.6851747035980225, "y": 10.30562686920166, "text": "[Page no. 9] \"lengths within a request batch. (3) Fused block copy. Block copy operations, issued by the copy-on-write mechanism, may operate on discontinuous blocks. This can lead to numerous invocations of small data movements if we use the cudaMemcpyAsync API. To mitigate the overhead, we implement a kernel that batches the copy operations for different blocks into a single kernel launch. 5.2 Supporting Various Decoding Algorithms vLLM implements various decoding algorithms using three key methods: fork, append, and free. The fork method creates a new sequence from an existing one. The append method appends a new token to the sequence. Finally, the free method deletes the sequence. For instance, in paral- lel sampling, vLLM creates multiple output sequences from the single input sequence using the fork method. It then adds new tokens to these sequences in every iteration with append, and deletes sequences that meet a stopping condi- tion using free. The\""}, {"x": 6.152210235595703, "y": 4.813040256500244, "text": "[Page no. 10] \"same strategy is also applied in beam search and prefix sharing by vLLM. We believe future decod- ing algorithms can also be supported by combining these methods. 6 Evaluation In this section, we evaluate the performance of vLLM under a variety of workloads. 9  0.0 0.5 1.0 1.5 2.0 Request rate (req/s) (a) OPT-13B, 1 GPU, ShareGPT 0.0 0.5 1.0 0.0 0.2 0.4 0.6 0.8 1.0 Request rate (req/s) (b) OPT-66B, 4 GPUs, ShareGPT 0.0 0.5 1.0 0.0 0.5 1.0 1.5 2.0 2.5 Request rate (req/s) (c) OPT-175B, 8 GPUs, ShareGPT 0.0 0.5 1.0 Normalized latency (s/token) FasterTransformer Orca (Max) Orca (Pow2) Orca (Oracle) vLLM 0 10 20 30 Request rate (req/s) (d) OPT-13B, 1 GPU, Alpaca 0.0 0.5 1.0 0 5 10 15 20 Request rate (req/s) (e) OPT-66B, 4 GPUs, Alpaca 0.0 0.5 1.0 0 5 10 15 20 Request rate (req/s) (f) OPT-175B, 8 GPUs, Alpaca\""}, {"x": 5.1075239181518555, "y": 3.5474162101745605, "text": "[Page no. 10] \"0.0 0.5 1.0 Normalized latency (s/token) Figure 12. Single sequence generation with OPT models on the ShareGPT and Alpaca dataset Orca (Max) Orca (Pow2) Orca (Oracle) vLLM 0 5 10 15 20 25 30 35 # Batched requests 7.00 9.81 13.62 30.42 (a) ShareGPT Orca (Max) Orca (Pow2) Orca (Oracle) vLLM 0 25 50 75 100 125 150 # Batched requests 7.00 43.24 72.75 132.44 (b) Alpaca Figure 13. Average number of batched requests when serv- ing OPT-13B for the ShareGPT (2 reqs/s) and Alpaca (30 reqs/s) traces. 6.1 Experimental Setup Model and server configurations. We use OPT [62] mod- els with 13B, 66B, and 175B parameters and LLaMA [52] with 13B parameters for our evaluation. 13B and 66B are popular sizes for LLMs as shown in an LLM leaderboard [38], while 175B is the size of the famous GPT-3 [5] model. For all of our experiments, we use A2\""}, {"x": 3.8663997650146484, "y": 0.9348433017730713, "text": "[Page no. 10] \"instances with NVIDIA A100 GPUs on Google Cloud Platform. The detailed model sizes and server configurations are shown in Table 1. Workloads. We synthesize workloads based on ShareGPT [51] and Alpaca [50] datasets, which contain input and output texts of real LLM services. The ShareGPT dataset is a collec- tion of user-shared conversations with ChatGPT [35]. The Alpaca dataset is an instruction dataset generated by GPT- 3.5 with self-instruct [57]. We tokenize the datasets and use their input and output lengths to synthesize client requests. As shown in Fig. 11, the ShareGPT dataset has 8.4\u00d7 longer input prompts and 5.8\u00d7 longer outputs on average than the Alpaca dataset, with higher variance. Since these datasets do not include timestamps, we generate request arrival times using Poisson distribution with different request rates. Baseline 1: FasterTransformer. FasterTransformer [31] is a distributed inference engine highly optimized for latency. As FasterTransformer does not have its\""}, {"x": 3.2801578044891357, "y": 2.755310297012329, "text": "[Page no. 10] \"own scheduler, we implement a custom scheduler with a dynamic batching mechanism similar to the existing serving systems such as Triton [30]. Specifically, we set a maximum batch size \ud835\udc35as large as possible for each experiment, according to the GPU memory capacity. The scheduler takes up to \ud835\udc35number of earliest arrived requests and sends the batch to FasterTrans- former for processing. Baseline 2: Orca. Orca [60] is a state-of-the-art LLM serving system optimized for throughput. Since Orca is not publicly available for use, we implement our own version of Orca. We assume Orca uses the buddy allocation algorithm to deter- mine the memory address to store KV cache. We implement three versions of Orca based on how much it over-reserves the space for request outputs: \u2022 Orca (Oracle). We assume the system has the knowledge of the lengths of the outputs that will be actually generated for the requests. This\""}, {"x": 4.874179363250732, "y": 2.7792398929595947, "text": "[Page no. 11] \"shows the upper-bound performance of Orca, which is infeasible to achieve in practice. \u2022 Orca (Pow2). We assume the system over-reserves the space for outputs by at most 2\u00d7. For example, if the true output length is 25, it reserves 32 positions for outputs. \u2022 Orca (Max). We assume the system always reserves the space up to the maximum sequence length of the model, i.e., 2048 tokens. Key metrics. We focus on serving throughput. Specifically, using the workloads with different request rates, we mea- sure normalized latency of the systems, the mean of every request\u2019s end-to-end latency divided by its output length, as in Orca [60]. A high-throughput serving system should retain low normalized latency against high request rates. For most experiments, we evaluate the systems with 1-hour traces. As an exception, we use 15-minute traces for the OPT-175B model due to the cost limit. 10  0 5 10\""}, {"x": 6.091763973236084, "y": 4.414106845855713, "text": "[Page no. 11] \"15 Request rate (req/s) (a) parallel generation (parallel size = 2) 0.0 0.5 1.0 0 2 4 6 8 10 Request rate (req/s) (b) parallel generation (parallel size = 4) 0.0 0.5 1.0 0 2 4 6 Request rate (req/s) (c) parallel generation (parallel size = 6) 0.0 0.5 1.0 Normalized latency (s/token) Orca (Max) Orca (Pow2) Orca (Oracle) vLLM 0 5 10 15 Request rate (req/s) (d) beam search (beam width = 2) 0.0 0.5 1.0 0 2 4 6 8 10 Request rate (req/s) (e) beam search (beam width = 4) 0.0 0.5 1.0 0 2 4 6 Request rate (req/s) (f) beam search (beam width = 6) 0.0 0.5 1.0 Normalized latency (s/token) Figure 14. Parallel generation and beam search with OPT-13B on the Alpaca dataset. 6.2 Basic Sampling We evaluate the performance of vLLM with basic sampling (one sample per request) on three models and two\""}, {"x": 4.263957977294922, "y": 4.048803806304932, "text": "[Page no. 11] \"datasets. The first row of Fig. 12 shows the results on the ShareGPT dataset. The curves illustrate that as the request rate in- creases, the latency initially increases at a gradual pace but then suddenly explodes. This can be attributed to the fact that when the request rate surpasses the capacity of the serv- ing system, the queue length continues to grow infinitely and so does the latency of the requests. On the ShareGPT dataset, vLLM can sustain 1.7\u00d7\u20132.7\u00d7 higher request rates compared to Orca (Oracle) and 2.7\u00d7\u20138\u00d7 compared to Orca (Max), while maintaining similar laten- cies. This is because vLLM\u2019s PagedAttention can efficiently manage the memory usage and thus enable batching more requests than Orca. For example, as shown in Fig. 13a, for OPT-13B vLLM processes 2.2\u00d7 more requests at the same time than Orca (Oracle) and 4.3\u00d7 more requests than Orca (Max). Compared to FasterTransformer, vLLM can sustain\""}, {"x": 4.430120944976807, "y": 3.9165000915527344, "text": "[Page no. 11] \"up to 22\u00d7 higher request rates, as FasterTransformer does not utilize a fine-grained scheduling mechanism and inefficiently manages the memory like Orca (Max). The second row of Fig. 12 and Fig. 13b shows the results on the Alpaca dataset, which follows a similar trend to the ShareGPT dataset. One exception is Fig. 12 (f), where vLLM\u2019s advantage over Orca (Oracle) and Orca (Pow2) is less pro- nounced. This is because the model and server configuration for OPT-175B (Table 1) allows for large GPU memory space available to store KV cache, while the Alpaca dataset has short sequences. In this setup, Orca (Oracle) and Orca (Pow2) can also batch a large number of requests despite the inef- ficiencies in their memory management. As a result, the performance of the systems becomes compute-bound rather than memory-bound. 2 4 6 # Output sequences 0 4 8 12 Memory saving (%) 6.09 8.53 9.79\""}, {"x": 5.232757091522217, "y": 5.600216388702393, "text": "[Page no. 11] \"(a) Parallel sampling 2 4 6 Beam width 0 20 40 60 Memory saving (%) 37.56 53.13 55.16 (b) Beam search Figure 15. Average amount of memory saving from sharing KV blocks, when serving OPT-13B for the Alpaca trace. 6.3 Parallel Sampling and Beam Search We evaluate the effectiveness of memory sharing in Page- dAttention with two popular sampling methods: parallel sampling and beam search. In parallel sampling, all paral- lel sequences in a request can share the KV cache for the prompt. As shown in the first row of Fig. 14, with a larger number of sequences to sample, vLLM brings more improve- ment over the Orca baselines. Similarly, the second row of Fig. 14 shows the results for beam search with different beam widths. Since beam search allows for more sharing, vLLM demonstrates even greater performance benefits. The im- provement of vLLM over Orca (Oracle) on OPT-13B and\""}, {"x": 5.167593955993652, "y": 5.458895206451416, "text": "[Page no. 12] \"the Alpaca dataset goes from 1.3\u00d7 in basic sampling to 2.3\u00d7 in beam search with a width of 6. Fig. 15 plots the amount of memory saving, computed by the number of blocks we saved by sharing divided by the number of total blocks without sharing. We show 6.1% - 9.8% memory saving on parallel sampling and 37.6% - 55.2% on beam search. In the same experiments with the ShareGPT dataset, we saw 16.2% - 30.5% memory saving on parallel sampling and 44.3% - 66.3% on beam search. 6.4 Shared prefix We explore the effectiveness of vLLM for the case a prefix is shared among different input prompts, as illustrated in 11  0 20 40 Request rate (req/s) (a) 1-shot prefix prompt 0.0 0.5 1.0 0 20 40 Request rate (req/s) (b) 5-shot prefix prompt 0.0 0.5 1.0 Normalized latency (s/token) Orca (Oracle) vLLM Figure 16. Translation workload where\""}, {"x": 3.2966928482055664, "y": 5.765412330627441, "text": "[Page no. 12] \"the input prompts share a common prefix. The prefix includes (a) 1 example with 80 tokens or (b) 5 examples with 341 tokens. 0.0 0.2 0.4 0.6 0.8 Request rate (req/s) 0.0 0.5 1.0 Normalized latency (s/token) Orca (Max) Orca (Pow2) Orca (Oracle) vLLM Figure 17. Performance on chatbot workload. Fig. 10. For the model, we use LLaMA-13B [52], which is mul- tilingual. For the workload, we use the WMT16 [4] English- to-German translation dataset and synthesize two prefixes that include an instruction and a few translation examples. The first prefix includes a single example (i.e., one-shot) while the other prefix includes 5 examples (i.e., few-shot). As shown in Fig. 16 (a), vLLM achieves 1.67\u00d7 higher through- put than Orca (Oracle) when the one-shot prefix is shared. Furthermore, when more examples are shared (Fig. 16 (b)), vLLM achieves 3.58\u00d7 higher throughput than Orca (Oracle). 6.5 Chatbot A chatbot [8, 19,\""}, {"x": 3.298344135284424, "y": 5.508228778839111, "text": "[Page no. 12] \"35] is one of the most important applications of LLMs. To implement a chatbot, we let the model generate a response by concatenating the chatting history and the last user query into a prompt. We synthesize the chatting history and user query using the ShareGPT dataset. Due to the limited context length of the OPT-13B model, we cut the prompt to the last 1024 tokens and let the model generate at most 1024 tokens. We do not store the KV cache between different conversation rounds as doing this would occupy the space for other requests between the conversation rounds. Fig. 17 shows that vLLM can sustain 2\u00d7 higher request rates compared to the three Orca baselines. Since the ShareGPT dataset contains many long conversations, the input prompts for most requests have 1024 tokens. Due to the buddy allo- cation algorithm, the Orca baselines reserve the space for 1024 tokens for\""}, {"x": 1.640852928161621, "y": 3.6915314197540283, "text": "[Page no. 12] \"the request outputs, regardless of how they predict the output lengths. For this reason, the three Orca baselines behave similarly. In contrast, vLLM can effectively 64 128 256 Context length 0 50 100 150 200 250 Kernel latency (us) vLLM (bs 8) FT (bs 8) vLLM (bs 32) FT (bs 32) (a) Latency of attention kernels. 1 2 4 8 16 32 64 128 256 Block size 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 Normalized latency (s/token) ShareGPT Alpaca (b) End-to-end latency with dif- ferent block sizes. Figure 18. Ablation experiments. handle the long prompts, as PagedAttention resolves the problem of memory fragmentation and reservation. 7 Ablation Studies In this section, we study various aspects of vLLM and evalu- ate the design choices we make with ablation experiments. 7.1 Kernel Microbenchmark The dynamic block mapping in PagedAttention affects the performance of the GPU operations involving the stored KV cache,\""}, {"x": 1.3790297508239746, "y": 3.892725706100464, "text": "[Page no. 12] \"i.e., block read/writes and attention. Compared to the existing systems, our GPU kernels (\u00a75) involve extra over- heads of accessing the block table, executing extra branches, and handling variable sequence lengths. As shown in Fig. 18a, this leads to 20\u201326% higher attention kernel latency, com- pared to the highly-optimized FasterTransformer implemen- tation. We believe the overhead is small as it only affects the attention operator but not the other operators in the model, such as Linear. Despite the overhead, PagedAttention makes vLLM significantly outperform FasterTransformer in end-to-end performance (\u00a76). 7.2 Impact of Block Size The choice of block size can have a substantial impact on the performance of vLLM. If the block size is too small, vLLM may not fully utilize the GPU\u2019s parallelism for reading and processing KV cache. If the block size is too large, inter- nal fragmentation increases and the probability of sharing decreases. In Fig. 18b,\""}, {"x": 4.037370204925537, "y": 7.9103264808654785, "text": "[Page no. 13] \"we evaluate the performance of vLLM with dif- ferent block sizes, using the ShareGPT and Alpaca traces with basic sampling under fixed request rates. In the ShareGPT trace, block sizes from 16 to 128 lead to the best performance. In the Alpaca trace, while the block size 16 and 32 work well, larger block sizes significantly degrade the performance since the sequences become shorter than the block sizes. In practice, we find that the block size 16 is large enough to efficiently utilize the GPU and small enough to avoid signifi- cant internal fragmentation in most workloads. Accordingly, vLLM sets its default block size as 16. 12  1 2 4 8 16 32 64 128 256 Block size 0 20 40 60 80 100 120 140 Time (ms) Recompute Swap in Swap out Swap in + out (a) Microbenchmark 1 2 4 8 16 32 64 128 256 Block\""}, {"x": 4.027594566345215, "y": 7.880425930023193, "text": "[Page no. 13] \"size 0.0 0.5 1.0 1.5 2.0 2.5 Normalized latency (s/token) Recompute Swap (b) End-to-end performance Figure 19. (a) Overhead of recomputation and swapping for different block sizes. (b) Performance when serving OPT-13B with the ShareGPT traces at the same request rate. 7.3 Comparing Recomputation and Swapping vLLM supports both recomputation and swapping as its re- covery mechanisms. To understand the tradeoffs between the two methods, we evaluate their end-to-end performance and microbenchmark their overheads, as presented in Fig. 19. Our results reveal that swapping incurs excessive overhead with small block sizes. This is because small block sizes often result in numerous small data transfers between CPU and GPU, which limits the effective PCIe bandwidth. In contrast, the overhead of recomputation remains constant across dif- ferent block sizes, as recomputation does not utilize the KV blocks. Thus, recomputation is more efficient when the block size is small, while swapping is more\""}, {"x": -3.3200082778930664, "y": 1.3828418254852295, "text": "[Page no. 13] \"efficient when the block size is large, though recomputation overhead is never higher than 20% of swapping\u2019s latency. For medium block sizes from 16 to 64, the two methods exhibit comparable end-to-end performance. 8 Discussion Applying the virtual memory and paging technique to other GPU workloads. The idea of virtual memory and paging is effective for managing the KV cache in LLM serving because the workload requires dynamic memory allocation (since the output length is not known a priori) and its perfor- mance is bound by the GPU memory capacity. However, this does not generally hold for every GPU workload. For exam- ple, in DNN training, the tensor shapes are typically static, and thus memory allocation can be optimized ahead of time. For another example, in serving DNNs that are not LLMs, an increase in memory efficiency may not result in any per- formance improvement since the performance is primarily\""}, {"x": -2.824977397918701, "y": 5.066324234008789, "text": "[Page no. 13] \"compute-bound. In such scenarios, introducing the vLLM\u2019s techniques may rather degrade the performance due to the extra overhead of memory indirection and non-contiguous block memory. However, we would be excited to see vLLM\u2019s techniques being applied to other workloads with similar properties to LLM serving. LLM-specific optimizations in applying virtual mem- ory and paging. vLLM re-interprets and augments the idea of virtual memory and paging by leveraging the application- specific semantics. One example is vLLM\u2019s all-or-nothing swap-out policy, which exploits the fact that processing a request requires all of its corresponding token states to be stored in GPU memory. Another example is the recomputa- tion method to recover the evicted blocks, which is not feasi- ble in OS. Besides, vLLM mitigates the overhead of memory indirection in paging by fusing the GPU kernels for memory access operations with those for other operations such as attention. 9 Related Work General model\""}, {"x": 2.934896230697632, "y": -1.8884927034378052, "text": "[Page no. 13] \"serving systems. Model serving has been an active area of research in recent years, with numerous systems proposed to tackle diverse aspects of deep learning model deployment. Clipper [11], TensorFlow Serving [33], Nexus [45], InferLine [10], and Clockwork [20] are some earlier general model serving systems. They study batch- ing, caching, placement, and scheduling for serving single or multiple models. More recently, DVABatch [12] intro- duces multi-entry multi-exit batching. REEF [21] and Shep- herd [61] propose preemption for serving. AlpaServe [28] utilizes model parallelism for statistical multiplexing. How- ever, these general systems fail to take into account the auto- regressive property and token state of LLM inference, result- ing in missed opportunities for optimization. Specialized serving systems for transformers. Due to the significance of the transformer architecture, numerous specialized serving systems for it have been developed. These systems utilize GPU kernel optimizations [1, 29, 31, 56], ad- vanced batching mechanisms\""}, {"x": 3.0274641513824463, "y": 3.278022527694702, "text": "[Page no. 13] \"[14, 60], model parallelism [1, 41, 60], and parameter sharing [64] for efficient serving. Among them, Orca [60] is most relevant to our approach. Comparison to Orca. The iteration-level scheduling in Orca [60] and PagedAttention in vLLM are complementary techniques: While both systems aim to increase the GPU utilization and hence the throughput of LLM serving, Orca achieves it by scheduling and interleaving the requests so that more requests can be processed in parallel, while vLLM is doing so by increasing memory utilization so that the working sets of more requests fit into memory. By reducing memory fragmentation and enabling sharing, vLLM runs more requests in a batch in parallel and achieves a 2-4\u00d7 speedup compared to Orca. Indeed, the fine-grained sched- uling and interleaving of the requests like in Orca makes memory management more challenging, making the tech- niques proposed in vLLM even more crucial. Memory optimizations. The widening\""}, {"x": 0.1765996366739273, "y": 3.0989060401916504, "text": "[Page no. 14] \"gap between the compute capability and memory capacity of accelerators has caused memory to become a bottleneck for both training and inference. Swapping [23, 42, 55], recomputation [7, 24] and their combination [40] have been utilized to reduce the peak memory of training. Notably, FlexGen [46] studies how to swap weights and token states for LLM inference with 13  limited GPU memory, but it does not target the online serv- ing settings. OLLA [48] optimizes the lifetime and location of tensors to reduce fragmentation, but it does not do fine- grained block-level management or online serving. FlashAt- tention [13] applies tiling and kernel optimizations to reduce the peak memory of attention computation and reduce I/O costs. This paper introduces a new idea of block-level mem- ory management in the context of online serving. 10 Conclusion This paper proposes PagedAttention, a new attention algo- rithm that allows attention keys and\""}, {"x": -0.7535168528556824, "y": 5.359230041503906, "text": "[Page no. 14] \"values to be stored in non-contiguous paged memory, and presents vLLM, a high-throughput LLM serving system with efficient mem- ory management enabled by PagedAttention. Inspired by operating systems, we demonstrate how established tech- niques, such as virtual memory and copy-on-write, can be adapted to efficiently manage KV cache and handle various decoding algorithms in LLM serving. Our experiments show that vLLM achieves 2-4\u00d7 throughput improvements over the state-of-the-art systems. Acknowledgement We would like to thank Xiaoxuan Liu, Zhifeng Chen, Yan- ping Huang, anonymous SOSP reviewers, and our shepherd, Lidong Zhou, for their insightful feedback. This research is partly supported by gifts from Andreessen Horowitz, Anyscale, Astronomer, Google, IBM, Intel, Lacework, Microsoft, Mo- hamed Bin Zayed University of Artificial Intelligence, Sam- sung SDS, Uber, and VMware. References [1] Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia Zhang, Am- mar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase,\""}, {"x": 5.139362812042236, "y": -5.6959004402160645, "text": "[Page no. 14] \"et al. 2022. DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale. arXiv preprint arXiv:2207.00032 (2022). [2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450 (2016). [3] Yoshua Bengio, R\u00e9jean Ducharme, and Pascal Vincent. 2000. A neural probabilistic language model. Advances in neural information process- ing systems 13 (2000). [4] Ond rej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Gra- ham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Aurelie Neveol, Mariana Neves, Martin Popel, Matt Post, Raphael Rubino, Car- olina Scarton, Lucia Specia, Marco Turchi, Karin Verspoor, and Marcos Zampieri. 2016. Findings of the 2016 Conference on Machine Trans- lation. In Proceedings of the First Conference on Machine Translation. Association for Computational Linguistics, Berlin, Germany, 131\u2013198. http://www.aclweb.org/anthology/W/W16/W16-2301 [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\""}, {"x": 4.620954990386963, "y": -4.83750057220459, "text": "[Page no. 14] \"Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877\u20131901. [6] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021). [7] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174 (2016). [8] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality. https://lmsys. org/blog/2023-03-30-vicuna/ [9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm:\""}, {"x": 3.490128517150879, "y": -1.6220769882202148, "text": "[Page no. 14] \"Scaling lan- guage modeling with pathways. arXiv preprint arXiv:2204.02311 (2022). [10] Daniel Crankshaw, Gur-Eyal Sela, Xiangxi Mo, Corey Zumar, Ion Stoica, Joseph Gonzalez, and Alexey Tumanov. 2020. InferLine: latency- aware provisioning and scaling for prediction serving pipelines. In Proceedings of the 11th ACM Symposium on Cloud Computing. 477\u2013491. [11] Daniel Crankshaw, Xin Wang, Guilio Zhou, Michael J Franklin, Joseph E Gonzalez, and Ion Stoica. 2017. Clipper: A Low-Latency Online Prediction Serving System. In 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17). 613\u2013627. [12] Weihao Cui, Han Zhao, Quan Chen, Hao Wei, Zirui Li, Deze Zeng, Chao Li, and Minyi Guo. 2022. DVABatch: Diversity-aware Multi- Entry Multi-Exit Batching for Efficient Processing of DNN Services on GPUs. In 2022 USENIX Annual Technical Conference (USENIX ATC 22). 183\u2013198. [13] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances\""}, {"x": 2.0850324630737305, "y": -3.647714853286743, "text": "[Page no. 14] \"in Neural Information Processing Systems 35 (2022), 16344\u201316359. [14] Jiarui Fang, Yang Yu, Chengduo Zhao, and Jie Zhou. 2021. TurboTrans- formers: an efficient GPU serving system for transformer models. In Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming. 389\u2013402. [15] FastAPI. 2023. FastAPI. https://github.com/tiangolo/fastapi. [16] Pin Gao, Lingfan Yu, Yongwei Wu, and Jinyang Li. 2018. Low latency rnn inference with cellular batching. In Proceedings of the Thirteenth EuroSys Conference. 1\u201315. [17] Amir Gholami, Zhewei Yao, Sehoon Kim, Michael W Mahoney, and Kurt Keutzer. 2021. Ai and memory wall. RiseLab Medium Post 1 (2021), 6. [18] Github. 2022. https://github.com/features/copilot [19] Google. 2023. https://bard.google.com/ [20] Arpan Gujarati, Reza Karimi, Safya Alzayat, Wei Hao, Antoine Kauf- mann, Ymir Vigfusson, and Jonathan Mace. 2020. Serving {DNNs} like Clockwork: Performance Predictability from the Bottom Up. In 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20). 443\u2013462. [21]\""}, {"x": 0.9174222946166992, "y": -3.9694933891296387, "text": "[Page no. 15] \"Mingcong Han, Hanze Zhang, Rong Chen, and Haibo Chen. 2022. Microsecond-scale Preemption for Concurrent {GPU- accelerated}{DNN} Inferences. In 16th USENIX Symposium on Oper- ating Systems Design and Implementation (OSDI 22). 539\u2013558. [22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition. 770\u2013778. [23] Chien-Chin Huang, Gu Jin, and Jinyang Li. 2020. Swapadvisor: Push- ing deep learning beyond the gpu memory limit via smart swapping. In Proceedings of the Twenty-Fifth International Conference on Archi- tectural Support for Programming Languages and Operating Systems. 1341\u20131355. [24] Paras Jain, Ajay Jain, Aniruddha Nrusimha, Amir Gholami, Pieter Abbeel, Joseph Gonzalez, Kurt Keutzer, and Ion Stoica. 2020. Check- mate: Breaking the memory wall with optimal tensor rematerialization. 14  Proceedings of Machine Learning and Systems 2 (2020), 497\u2013511. [25] Tom Kilburn, David BG Edwards, Michael J Lanigan,\""}, {"x": 4.74251127243042, "y": -2.800294876098633, "text": "[Page no. 15] \"and Frank H Sumner. 1962. One-level storage system. IRE Transactions on Electronic Computers 2 (1962), 223\u2013235. [26] Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691 (2021). [27] Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing contin- uous prompts for generation. arXiv preprint arXiv:2101.00190 (2021). [28] Zhuohan Li, Lianmin Zheng, Yinmin Zhong, Vincent Liu, Ying Sheng, Xin Jin, Yanping Huang, Zhifeng Chen, Hao Zhang, Joseph E Gonzalez, et al. 2023. AlpaServe: Statistical Multiplexing with Model Parallelism for Deep Learning Serving. arXiv preprint arXiv:2302.11665 (2023). [29] Lingxiao Ma, Zhiqiang Xie, Zhi Yang, Jilong Xue, Youshan Miao, Wei Cui, Wenxiang Hu, Fan Yang, Lintao Zhang, and Lidong Zhou. 2020. Rammer: Enabling holistic deep learning compiler optimizations with rtasks. In Proceedings of the 14th USENIX Conference on Operating Systems Design and Implementation. 881\u2013897. [30] NVIDIA. [n. d.]. Triton Inference Server. https://developer.nvidia.com/ nvidia-triton-inference-server.\""}, {"x": 3.6782920360565186, "y": -0.05748811736702919, "text": "[Page no. 15] \"[31] NVIDIA. 2023. FasterTransformer. https://github.com/NVIDIA/ FasterTransformer. [32] NVIDIA. 2023. NCCL: The NVIDIA Collective Communication Library. https://developer.nvidia.com/nccl. [33] Christopher Olston, Noah Fiedel, Kiril Gorovoy, Jeremiah Harmsen, Li Lao, Fangwei Li, Vinu Rajashekhar, Sukriti Ramesh, and Jordan Soyke. 2017. Tensorflow-serving: Flexible, high-performance ml serving. arXiv preprint arXiv:1712.06139 (2017). [34] OpenAI. 2020. https://openai.com/blog/openai-api [35] OpenAI. 2022. https://openai.com/blog/chatgpt [36] OpenAI. 2023. https://openai.com/blog/custom-instructions-for- chatgpt [37] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL] [38] LMSYS ORG. 2023. Chatbot Arena Leaderboard Week 8: Introduc- ing MT-Bench and Vicuna-33B. https://lmsys.org/blog/2023-06-22- leaderboard/. [39] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning library. Advances in neural informa- tion processing systems 32 (2019). [40] Shishir G Patil, Paras Jain, Prabal Dutta, Ion Stoica, and Joseph Gon- zalez. 2022. POET: Training Neural Networks on Tiny Devices with Integrated Rematerialization and\""}, {"x": 3.8457071781158447, "y": -5.620334625244141, "text": "[Page no. 15] \"Paging. In International Conference on Machine Learning. PMLR, 17573\u201317583. [41] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. 2022. Efficiently Scaling Transformer Inference. arXiv preprint arXiv:2211.05102 (2022). [42] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. 2021. ZeRO-Offload: Democratizing Billion-Scale Model Training.. In USENIX Annual Technical Conference. 551\u2013564. [43] Reuters. 2023. https://www.reuters.com/technology/tech-giants-ai- like-bing-bard-poses-billion-dollar-search-problem-2023-02-22/ [44] Amazon Web Services. 2023. https://aws.amazon.com/bedrock/ [45] Haichen Shen, Lequn Chen, Yuchen Jin, Liangyu Zhao, Bingyu Kong, Matthai Philipose, Arvind Krishnamurthy, and Ravi Sundaram. 2019. Nexus: A GPU cluster engine for accelerating DNN-based video anal- ysis. In Proceedings of the 27th ACM Symposium on Operating Systems Principles. 322\u2013337. [46] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E Gon- zalez, et al.\""}, {"x": 4.5513739585876465, "y": -4.384791851043701, "text": "[Page no. 15] \"2023. High-throughput Generative Inference of Large Language Models with a Single GPU. arXiv preprint arXiv:2303.06865 (2023). [47] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-lm: Training multi- billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053 (2019). [48] Benoit Steiner, Mostafa Elhoushi, Jacob Kahn, and James Hegarty. 2022. OLLA: Optimizing the Lifetime and Location of Arrays to Reduce the Memory Usage of Neural Networks. (2022). https://doi.org/10.48550/ arXiv.2210.12924 [49] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to se- quence learning with neural networks. Advances in neural information processing systems 27 (2014). [50] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford Alpaca: An Instruction-following LLaMA model. https:// github.com/tatsu-lab/stanford_alpaca. [51] ShareGPT Team. 2023. https://sharegpt.com/ [52] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie- Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re,\""}, {"x": 4.977713108062744, "y": -3.3905513286590576, "text": "[Page no. 15] \"Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023). [53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. At- tention is all you need. Advances in neural information processing systems 30 (2017). [54] Jing Wang, Youyou Lu, Qing Wang, Minhui Xie, Keji Huang, and Jiwu Shu. 2022. Pacman: An Efficient Compaction Approach for {Log- Structured}{Key-Value} Store on Persistent Memory. In 2022 USENIX Annual Technical Conference (USENIX ATC 22). 773\u2013788. [55] Linnan Wang, Jinmian Ye, Yiyang Zhao, Wei Wu, Ang Li, Shuai- wen Leon Song, Zenglin Xu, and Tim Kraska. 2018. Superneurons: Dy- namic GPU memory management for training deep neural networks. In Proceedings of the 23rd ACM SIGPLAN symposium on principles and practice of parallel programming. 41\u201353. [56] Xiaohui Wang, Ying Xiong, Yang Wei, Mingxuan Wang, and Lei\""}, {"x": 5.486928939819336, "y": -5.768486022949219, "text": "[Page no. 15] \"Li. 2021. LightSeq: A High Performance Inference Library for Transform- ers. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies: Industry Papers. 113\u2013120. [57] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-Instruct: Aligning Language Model with Self Generated Instructions. arXiv preprint arXiv:2212.10560 (2022). [58] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, et al. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations. 38\u201345. [59] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google\u2019s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint\""}, {"x": 2.224308729171753, "y": -1.6957920789718628, "text": "[Page no. 16] \"arXiv:1609.08144 (2016). [60] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. 2022. Orca: A Distributed Serving System for {Transformer-Based} Generative Models. In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22). 521\u2013538. [61] Hong Zhang, Yupeng Tang, Anurag Khandelwal, and Ion Stoica. 2023. SHEPHERD: Serving DNNs in the Wild. In 20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23). USENIX As- sociation, Boston, MA, 787\u2013808. https://www.usenix.org/conference/ nsdi23/presentation/zhang-hong 15  [62] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 (2022). [63] Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Eric P Xing, et al. 2022. Alpa: Automating Inter-and Intra-Operator Parallelism for Distributed Deep Learning. In 16th USENIX Symposium\""}, {"x": 1.8091955184936523, "y": -1.617553949356079, "text": "[Page no. 16] \"on Operating Systems Design and Implementation (OSDI 22). 559\u2013578. [64] Zhe Zhou, Xuechao Wei, Jiejing Zhang, and Guangyu Sun. 2022. PetS: A Unified Framework for Parameter-Efficient Transformers Serving. In 2022 USENIX Annual Technical Conference (USENIX ATC 22). 489\u2013504. 16\""}]}}