{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import requests\n",
    "from pprint import pprint\n",
    "\n",
    "try:\n",
    "    import pymupdf as fitz  # available with v1.24.3\n",
    "except ImportError:\n",
    "    import fitz\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from fitz import Document as FitzDocument\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"./pdfs/outerbounds-brief.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = fitz.open(pdf_path)\n",
    "assert doc.is_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of pages: {doc.page_count}\")\n",
    "print(f\"Metadata: \", end=\"\")\n",
    "pprint(doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(doc.get_toc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pdf_utils.py\n",
    "try:\n",
    "    import pymupdf as fitz  # available with v1.24.3\n",
    "except ImportError:\n",
    "    import fitz\n",
    "\n",
    "import re\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def pdf_to_text(path, start_page=1, end_page=None):\n",
    "    doc = fitz.open(path)\n",
    "    total_pages = doc.page_count\n",
    "    if end_page is None:\n",
    "        end_page = total_pages\n",
    "    text_list = []\n",
    "    for i in range(start_page - 1, end_page):\n",
    "        text = doc.load_page(i).get_text(\"text\")\n",
    "        text = preprocess(text)\n",
    "        text_list.append({\"content\": text, \"page\": i + 1})\n",
    "    doc.close()\n",
    "    return text_list\n",
    "\n",
    "\n",
    "def text_to_chunks(texts, word_length=150, start_page=1):\n",
    "    text_toks = [(t[\"content\"].split(\" \"), t[\"page\"]) for t in texts]\n",
    "    chunks = []\n",
    "\n",
    "    for idx, words_and_page in enumerate(text_toks):\n",
    "        words = words_and_page[0]\n",
    "        page = words_and_page[1]\n",
    "        for i in range(0, len(words), word_length):\n",
    "            chunk = words[i : i + word_length]\n",
    "            if (\n",
    "                (i + word_length) > len(words)\n",
    "                and (len(chunk) < word_length)\n",
    "                and (len(text_toks) != (idx + 1))\n",
    "            ):\n",
    "                # text_toks[idx + 1] = chunk + text_toks[idx + 1]\n",
    "                text_toks[idx + 1] = (\n",
    "                    chunk + text_toks[idx + 1][0],\n",
    "                    text_toks[idx + 1][1],\n",
    "                )\n",
    "                continue\n",
    "            chunk = \" \".join(chunk).strip()\n",
    "            chunk = f\"[Page no. {idx+start_page}]\" + \" \" + '\"' + chunk + '\"'\n",
    "            chunks.append((chunk, page))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf_utils import pdf_to_text, text_to_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile semantic_search.py\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import alt\n",
    "import json\n",
    "\n",
    "TEXT_EMBEDDING_MODEL_INFO = {\n",
    "    \"model_name\": \"all-MiniLM-L6-v2\",\n",
    "    \"model_framework\": \"sentence-transformers\",\n",
    "    \"pretrained_model_provider\": \"Hugging Face\",\n",
    "    \"use_case\": \"text-semantic-search\",\n",
    "}\n",
    "\n",
    "\n",
    "class SemanticSearchModel:\n",
    "    \"\"\"\n",
    "    Manager for a semantic search model.\n",
    "\n",
    "    args:\n",
    "        None\n",
    "\n",
    "    methods:\n",
    "        fit(data: List[str], batch: int, n_neighbors: int) -> None:\n",
    "            Fits the model M with the data.\n",
    "        _get_text_embedding(texts: List[str], batch: int) -> np.ndarray:\n",
    "            Returns the embeddings of the text.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.embedding_model = SentenceTransformer(\n",
    "            TEXT_EMBEDDING_MODEL_INFO[\"model_name\"]\n",
    "        )\n",
    "        self.fitted = False\n",
    "\n",
    "    def _get_text_embedding(self, texts, batch_size=1000):\n",
    "        \"\"\"\n",
    "        Gather a stack of embedded texts, packed batch_size at a time.\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        n_texts = len(texts)\n",
    "        for batch_start_idx in range(0, n_texts, batch_size):\n",
    "            text_batch = texts[batch_start_idx : (batch_start_idx + batch_size)]\n",
    "            embedding_batch = self.embedding_model.encode(text_batch)\n",
    "            embeddings.append(embedding_batch)\n",
    "        print(\"[DEBUG] Embedding batches:\", len(embeddings))\n",
    "        embeddings = np.vstack(embeddings)\n",
    "        print(\"[DEBUG] Embedding reshaped:\", embeddings.shape)\n",
    "        return embeddings\n",
    "\n",
    "    def fit(self, data, batch_size=1000, n_neighbors=6):\n",
    "        \"\"\"\n",
    "        The only public method in this class.\n",
    "        Fits the model with the data when a new PDF is uploaded.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.embeddings = self._get_text_embedding(data, batch_size=batch_size)\n",
    "        n_neighbors = min(n_neighbors, len(self.embeddings))\n",
    "        print(\n",
    "            \"[DEBUG] Fitting Nearest Neighbors model with %s neighbors.\" % n_neighbors\n",
    "        )\n",
    "        self.nn = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "        self.nn.fit(self.embeddings)\n",
    "        print(\"[DEBUG] Fit complete.\")\n",
    "        self.fitted = True\n",
    "\n",
    "    def __call__(self, text, return_data=True):\n",
    "        \"\"\"\n",
    "        Inference time method.\n",
    "        Return the nearest neighbors of a new text.\n",
    "        \"\"\"\n",
    "        print(\"[DEBUG] Getting nearest neighbors of text:\", text)\n",
    "        embedding = self.embedding_model.encode([text])\n",
    "        print(\"[DEBUG] Embedding:\", embedding.shape)\n",
    "        neighbors = self.nn.kneighbors(embedding, return_distance=False)[0]\n",
    "        if return_data:\n",
    "            return [self.data[text_neighbs] for text_neighbs in neighbors]\n",
    "        else:\n",
    "            return neighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ls = pdf_to_text(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ls[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_to_chunks(text_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(chunks[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_search import SemanticSearchModel\n",
    "\n",
    "recommender = SemanticSearchModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender.fit([c[0] for c in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What does Outerbounds do?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topn_chunks = recommender(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topn_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\n",
    "prompt += \"search results:\\n\\n\"\n",
    "for c in topn_chunks:\n",
    "    prompt += c + \"\\n\\n\"\n",
    "\n",
    "# stolen: https://github.com/bhaskatripathi/pdfGPT/blob/main/api.py#L137C5-L146C6\n",
    "prompt += (\n",
    "    \"Instructions: Only reply to the query based on the search results given. \"\n",
    "    \"Cite each reference using [ Page Number ] notation \"\n",
    "    \"(every result has this number at the beginning). \"\n",
    "    \"Weave responses into a coherent and succinct paragraph. \"\n",
    "    \"Citation should be done in the same words that it refers to in Markdown. \"\n",
    "    \"Only include information found in the results and \"\n",
    "    \"Only answer what is asked. The answer should be short and concise. \"\n",
    "    \"Answer step-by-step. Include the page number in the most relevant citations. \"\n",
    "    \"Return a JSON object with the following format: \\n\\n\"\n",
    "    \"\\n\\n{\\n\"\n",
    "    f'  \"query\": \"{question}\",\\n'\n",
    "    '  \"answer\":'\n",
    "    \"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an elite machine learn...er. \"\n",
    "        + \"Discuss and connect the topics related to the search results, but do no not discuss other topics others.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=message_history,\n",
    "    response_format={\"type\": \"json_object\"},\n",
    "    max_tokens=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json.loads(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch remote data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "\n",
    "def download(url: str, path: str) -> FitzDocument:\n",
    "    subprocess.run(\n",
    "        [\"wget\", \"--user-agent\", \"Mozilla\", url, \"-O\", path],\n",
    "        stdout=subprocess.DEVNULL,\n",
    "        stderr=subprocess.DEVNULL,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"pdfs/llama2.pdf\"\n",
    "download(\"https://arxiv.org/pdf/2307.09288.pdf\", pdf_path)\n",
    "pdf = fitz.open(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ls = pdf_to_text(pdf_path)\n",
    "chunks = text_to_chunks(text_ls)\n",
    "recommender.fit([c[0] for c in chunks])\n",
    "\n",
    "question = \"What were major advances in Llama 2?\"\n",
    "topn_chunks = recommender(question)\n",
    "\n",
    "prompt = \"\"\n",
    "prompt += \"search results:\\n\\n\"\n",
    "for c in topn_chunks:\n",
    "    prompt += c + \"\\n\\n\"\n",
    "\n",
    "prompt += (\n",
    "    \"Instructions: Only reply to the query based on the search results given. \"\n",
    "    \"Cite each reference using [ Page Number ] notation \"\n",
    "    \"(every result has this number at the beginning). \"\n",
    "    \"Weave responses into a coherent and succinct paragraph. \"\n",
    "    \"Citation should be done in the same words that it refers to in Markdown. \"\n",
    "    \"Only include information found in the results and \"\n",
    "    \"Only answer what is asked. The answer should be short and concise. \"\n",
    "    \"Return a JSON object with the following format: \\n\\n\"\n",
    "    \"Answer step-by-step. Include the page number in the most relevant citations. \"\n",
    "    \"\\n\\n{\\n\"\n",
    "    f'  \"query\": \"{question}\",\\n'\n",
    "    '  \"answer\":'\n",
    "    \"\\n\"\n",
    ")\n",
    "\n",
    "message_history = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an elite professor specializing in machine learning. \"\n",
    "        + \"Discuss topics related to the search results, and no others.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt,\n",
    "    },\n",
    "]\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\", messages=message_history, response_format={\"type\": \"json_object\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
